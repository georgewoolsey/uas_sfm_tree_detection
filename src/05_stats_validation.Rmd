# Statistical Analysis: Validation{#stats_validation}

In this section, we'll evaluate the influence of the processing parameters on UAS-derived tree detection and monitoring. 

The UAS and Field validation data was built and described in [this section](#field_valid).

The objective of this study is to determine the influence of different structure from motion (SfM) software (e.g. Agisoft  Metashap, OpenDroneMap, Pix4D) and processing parameters on F-score which is a measure of overall tree detection performance.

All of the predictor variables of interest in this study are categorical (a.k.a. factor or nominal) while the predicted variables are metric and include F-score (ranges from 0-1) and error (e.g. MAPE, RMSE). This type of statistical analysis is described in the second edition of Kruschke's [*Doing Bayesian data analysis* (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) and here we will build a Bayesian approach based on [Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/). This analysis was greatly enhanced by [A. Solomon Kurz's ebook supplement](https://solomonkurz.netlify.app/book/) to [Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/).

For a more in-depth review of the traditional treatment of this sort of data structure called multifactor analysis of variance (ANOVA) compared to the Bayesian hierarchical generalization of the traditional ANOVA model used here see [this previous section](#stats_processing_time).

## Setup

first we're going to define a function to ingest a formula as text and separate it into multiple rows based on the number of characters for plotting

```{r}
# function to pull the formula for labeling below
get_frmla_text = function(frmla_chr, split_chrs = 100){

  cumsum_group = function(x, threshold) {
    cumsum = 0
    group = 1
    result = numeric()
    for (i in 1:length(x)) {
      cumsum = cumsum + x[i]
      if (cumsum > threshold) {
        group = group + 1
        cumsum = x[i]
      }
      result = c(result, group)
    }
    return (result)
  }
  
  r = stringr::str_sub(
    frmla_chr
    , # get the two column matrix of start end
      frmla_chr %>% 
        stringr::str_locate_all("\\+") %>% 
        .[[1]] %>% 
        dplyr::as_tibble() %>% 
        dplyr::select(start) %>% 
        dplyr::mutate(
          len = dplyr::coalesce(start-dplyr::lag(start),0)
          , ld = dplyr::coalesce(dplyr::lead(start)-1, stringr::str_length(frmla_chr))
          , cum = cumsum_group(len, split_chrs)
          , start = ifelse(dplyr::row_number()==1,1,start)
        ) %>% 
        dplyr::group_by(cum) %>% 
        dplyr::summarise(start = min(start), end = max(ld)) %>% 
        dplyr::ungroup() %>% 
        dplyr::select(-cum) %>% 
        as.matrix()
    ) %>% 
    stringr::str_squish() %>% 
    paste0(collapse = "\n")
  
  return(r)
}
```

## Summary Statistics

What is this data?

```{r}
# load data if needed
if(ls()[ls() %in% "ptcld_validation_data"] %>% length()==0){
 ptcld_validation_data = readr::read_csv("../data/ptcld_full_analysis_data.csv") %>% 
   dplyr::mutate(
      depth_maps_generation_quality = factor(
          depth_maps_generation_quality %>% 
            tolower() %>% 
            stringr::str_replace_all("ultrahigh", "ultra high")
          , ordered = TRUE
          , levels = c(
            "lowest"
            , "low"
            , "medium"
            , "high"
            , "ultra high"
          )
        ) %>% forcats::fct_rev()
      , depth_maps_generation_filtering_mode = factor(
          depth_maps_generation_filtering_mode %>% tolower()
          , ordered = TRUE
          , levels = c(
            "disabled"
            , "mild"
            , "moderate"
            , "aggressive"
          )
        ) %>% forcats::fct_rev()
    )
}
# replace 0 F-score with very small positive to run models
ptcld_validation_data = ptcld_validation_data %>% 
  dplyr::mutate(dplyr::across(
    .cols = tidyselect::ends_with("f_score")
    , .fns = ~ ifelse(.x==0,1e-4,.x)
  ))
# what is this data?
ptcld_validation_data %>% dplyr::glimpse()
# a row is unique by...
identical(
  nrow(ptcld_validation_data)
  , ptcld_validation_data %>% 
    dplyr::distinct(
      study_site, software
      , depth_maps_generation_quality
      , depth_maps_generation_filtering_mode
      , processing_attribute3 # need to align all by software so this will go away or be filled
    ) %>% 
    nrow()
)

```

Summary by metrics of interest

```{r}
sum_stats_dta = function(my_var){
  sum_fns = list(
    n = ~sum(ifelse(is.na(.x), 0, 1))  
    , min = ~min(.x, na.rm = TRUE)
    , max = ~max(.x, na.rm = TRUE)
    , mean = ~mean(.x, na.rm = TRUE)
    , median = ~median(.x, na.rm = TRUE)
    , sd = ~sd(.x, na.rm = TRUE)
  )
  # plot
  (
    ggplot(
      data = ptcld_validation_data %>% 
        dplyr::group_by(.data[[my_var]]) %>%
        dplyr::mutate(m = median(f_score))
      , mapping = aes(
        y = .data[[my_var]]
        , x = f_score, fill = m)
      ) +
      geom_violin(color = NA) + 
      geom_boxplot(width = 0.1, outlier.shape = NA, fill = NA, color = "black") +
      geom_rug() +
      scale_fill_viridis_c(option = "mako", begin = 0.3, end = 0.9, direction = -1) +
      labs(
        x = "F-score"
        , y = stringr::str_replace_all(my_var, pattern = "_", replacement = " ")
        , subtitle = stringr::str_replace_all(my_var, pattern = "_", replacement = " ") %>% 
          stringr::str_to_title()
      ) +
      theme_light() +
      theme(legend.position = "none")
  )
  # # summarize data
  # (
  # ptcld_validation_data %>% 
  #   dplyr::group_by(dplyr::across(dplyr::all_of(my_var))) %>%
  #   dplyr::summarise(
  #     dplyr::across(f_score, sum_fns)
  #     , .groups = 'drop_last'
  #   ) %>% 
  #   kableExtra::kbl() %>% 
  #   kableExtra::kable_styling()
  # )
}
# sum_stats_dta("software")
```

summarize for all variables of interest

```{r}
c("software", "study_site"
  , "depth_maps_generation_quality"
  , "depth_maps_generation_filtering_mode"
) %>% 
  purrr::map(sum_stats_dta)
```

## One Nominal Predictor{#f_one_pred_mod}

We'll start by exploring the influence of the depth map generation quality parameter on the SfM-derived tree detection performance based on the F-score.

### Summary Statistics

Summary statistics by group:

```{r}
ptcld_validation_data %>% 
  dplyr::group_by(depth_maps_generation_quality) %>% 
  dplyr::summarise(
    mean_f_score = mean(f_score, na.rm = T)
    # , med_f_score = median(f_score, na.rm = T)
    , sd_f_score = sd(f_score, na.rm = T)
    , n = dplyr::n()
  ) %>% 
  kableExtra::kbl(digits = 2, caption = "summary statistics: F-score by dense cloud quality") %>% 
  kableExtra::kable_styling()
```

### Bayesian{#f_one_pred_mod_bays}

[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) notes: 

> The terminology, "analysis of variance," comes from a decomposition of overall data variance into within-group variance and between-group variance (Fisher, 1925). Algebraically, the sum of squared deviations of the scores from their overall mean equals the sum of squared deviations of the scores from their respective group means plus the sum of squared deviations of the group means from the overall mean. In other words, the total variance can be partitioned into within-group variance plus between-group variance. Because one definition of the word "analysis" is separation into constituent parts, the term ANOVA accurately describes the underlying algebra in the traditional methods. That algebraic relation is not used in the hierarchical Bayesian approach presented here. The Bayesian method can estimate component variances, however. Therefore, the Bayesian approach is not ANOVA, but is analogous to ANOVA. (p. 556)

and see section 19 from [Kurz's ebook supplement](https://bookdown.org/content/3686/metric-predicted-variable-with-one-nominal-predictor.html)

The metric predicted variable with one nominal predictor variable model has the form:

\begin{align*}
y_{i}  &\sim \operatorname{Normal} \bigl(\mu_{i}, \sigma_{y} \bigr) \\
\mu_{i} &= \beta_0 + \sum_{j=1}^{J} \beta_{1[j]} x_{1[j]} \bigl(i\bigr) \\
\beta_{0}  &\sim \operatorname{Normal}(0,10) \\ 
\beta_{1[j]}  &\sim \operatorname{Normal}(0,\sigma_{\beta_{1}}) \\ 
\sigma_{\beta_{1}} &\sim {\sf uniform} (0,100) \\ 
\sigma_{y} &\sim {\sf uniform} (0,100) \\ 
\end{align*}

, where $j$ is the depth map generation quality setting corresponding to observation $i$

*to start, we'll use the default `brms::brm` prior settings which may not match those described in the model specification above* 

```{r}
brms_f_mod1 = brms::brm(
  formula = f_score ~ 1 + (1 | depth_maps_generation_quality)
  , data = ptcld_validation_data
  , family = brms::brmsfamily(family = "gaussian")
  , iter = 4000, warmup = 2000, chains = 4
  , cores = round(parallel::detectCores()/2)
  , file = paste0(rootdir, "/fits/brms_f_mod1")
)
```

check the trace plots for problems with convergence of the Markov chains

```{r}
plot(brms_f_mod1)
```

check the prior distributions

```{r}
# check priors
brms::prior_summary(brms_f_mod1) %>% 
  kableExtra::kbl() %>% 
  kableExtra::kable_styling()
```

The `brms::brm` model summary

```{r}
brms_f_mod1 %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | parameter == "sigma"
  ) %>%
  dplyr::mutate(
    parameter = parameter %>% 
      stringr::str_remove_all("b_depth_maps_generation_quality") %>% 
      stringr::str_remove_all("r_depth_maps_generation_quality")
  ) %>% 
  kableExtra::kbl(digits = 2, caption = "Bayesian one nominal predictor: F-score by dense cloud quality") %>% 
  kableExtra::kable_styling()
```

With the `stats::coef` function, we can get the group-level summaries in a "non-deflection" metric. In the model, the group means represented by $\beta_{1[j]}$ are deflections from overall baseline, such that the deflections sum to zero (see [Kruschke (2015, p.554)](https://sites.google.com/site/doingbayesiandataanalysis/)). Summaries of the group-specific deflections are available via the `brms::ranef` function.

```{r}
stats::coef(brms_f_mod1) %>%
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "group") %>% 
  dplyr::rename_with(
    .cols = -c("group")
    , .fn = ~ stringr::str_remove_all(.x, "depth_maps_generation_quality.")
  ) %>% 
  kableExtra::kbl(digits = 2, caption = "brms::brm model: F-score by dense cloud quality") %>% 
  kableExtra::kable_styling()
```

We can look at the model noise standard deviation $\sigma_y$

```{r}
# get formula
form_temp = brms_f_mod1$formula$formula[3] %>% 
  as.character() %>% get_frmla_text() %>% 
  stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
  stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
# extract the posterior draws
brms::as_draws_df(brms_f_mod1) %>% 
# plot
  ggplot(aes(x = sigma, y = 0)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21, point_size = 3
    , quantiles = 100
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\sigma_y$")
    , caption = form_temp
  ) +
  theme_light()
```

plot the posterior predictive distributions of the conditional means with the median F-score and the 95% highest posterior density interval (HDI)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(brms_f_mod1) %>% 
  dplyr::mutate(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_quality
      , fill = depth_maps_generation_quality
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "inferno", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "quality", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

we can also make pairwise comparisons

```{r}
# first we need to define the contrasts to make
contrast_list = 
  tidyr::crossing(
    x1 = unique(ptcld_validation_data$depth_maps_generation_quality)
    , x2 = unique(ptcld_validation_data$depth_maps_generation_quality)
  ) %>% 
  dplyr::mutate(
    dplyr::across(
      dplyr::everything()
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
  ) %>% 
  dplyr::filter(x1<x2) %>% 
  dplyr::arrange(x1,x2) %>% 
  dplyr::mutate(dplyr::across(dplyr::everything(), as.character)) %>% 
  purrr::transpose()

# contrast_list

# obtain posterior draws and calculate contrasts using tidybayes::compare_levels
brms_contrast_temp = 
  brms_f_mod1 %>% 
    tidybayes::spread_draws(r_depth_maps_generation_quality[depth_maps_generation_quality]) %>% 
    dplyr::mutate(
      depth_maps_generation_quality = depth_maps_generation_quality %>% 
        stringr::str_replace_all("\\.", " ") %>% 
        factor(
          levels = levels(ptcld_validation_data$depth_maps_generation_quality)
          , ordered = T
        )
    ) %>% 
    dplyr::rename(value = r_depth_maps_generation_quality) %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) 

# generate the contrast column for creating an ordered factor
brms_contrast_temp =
  brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = depth_maps_generation_quality
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$depth_maps_generation_quality, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = depth_maps_generation_quality %>% forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
  )

# median_hdi summary for coloring 
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::group_by(contrast) %>% 
  dplyr::mutate(
    # get median_hdi
    median_hdi_est = tidybayes::median_hdi(value)$y
    , median_hdi_lower = tidybayes::median_hdi(value)$ymin
    , median_hdi_upper = tidybayes::median_hdi(value)$ymax
    # check probability of contrast
    , pr_gt_zero = mean(value > 0) %>% 
        scales::percent(accuracy = 1)
    , pr_lt_zero = mean(value < 0) %>% 
        scales::percent(accuracy = 1)
    # check probability that this direction is true
    , is_diff_dir = dplyr::case_when(
      median_hdi_est >= 0 ~ value > 0
      , median_hdi_est < 0 ~ value < 0
    )
    , pr_diff = mean(is_diff_dir)
    # make a label
    , pr_diff_lab = dplyr::case_when(
      median_hdi_est > 0 ~ paste0(
        "Pr("
        , stringr::word(contrast, 1, sep = fixed("-")) %>% 
          stringr::str_squish()
        , ">"
        , stringr::word(contrast, 2, sep = fixed("-")) %>% 
          stringr::str_squish()
        , ")="
        , pr_diff %>% scales::percent(accuracy = 1)
      )
      , median_hdi_est < 0 ~ paste0(
        "Pr("
        , stringr::word(contrast, 2, sep = fixed("-")) %>% 
          stringr::str_squish()
        , ">"
        , stringr::word(contrast, 1, sep = fixed("-")) %>% 
          stringr::str_squish()
        , ")="
        , pr_diff %>% scales::percent(accuracy = 1)
      )
    )
    # make a SMALLER label
    , pr_diff_lab_sm = dplyr::case_when(
      median_hdi_est >= 0 ~ paste0(
        "Pr(>0)="
        , pr_diff %>% scales::percent(accuracy = 1)
      )
      , median_hdi_est < 0 ~ paste0(
        "Pr(<0)="
        , pr_diff %>% scales::percent(accuracy = 1)
      )
    )
    , pr_diff_lab_pos = dplyr::case_when(
      median_hdi_est > 0 ~ median_hdi_upper
      , median_hdi_est < 0 ~ median_hdi_lower
    ) * 1.09
    , sig_level = dplyr::case_when(
      pr_diff > 0.99 ~ 0
      , pr_diff > 0.95 ~ 1
      , pr_diff > 0.9 ~ 2
      , pr_diff > 0.8 ~ 3
      , T ~ 4
    ) %>% 
    factor(levels = c(0:4), labels = c(">99%","95%","90%","80%","<80%"), ordered = T)
  )
# what?
brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  dplyr::count(contrast, median_hdi_est, pr_diff_lab,pr_diff_lab_sm)
```

plot it

```{r}
# plot, finally
brms_contrast_temp %>% 
  ggplot(aes(x = value, y = contrast, fill = pr_diff)) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = c(0.5,0.95)
      # , slab_fill = "gray22", slab_alpha = 1
      , interval_color = "black", point_color = "black", point_fill = "black"
      , justification = -0.01
    ) +
    geom_vline(xintercept = 0, linetype = "dashed", color = "gray44") +
    geom_text(
      data = brms_contrast_temp %>% 
        dplyr::ungroup() %>% 
        dplyr::count(contrast, pr_diff_lab, pr_diff_lab_pos, pr_diff)
      , mapping = aes(x = pr_diff_lab_pos, label = pr_diff_lab)
      , vjust = -1, hjust = 0, size = 2.5
    ) +
    scale_fill_fermenter(
      n.breaks = 10, palette = "PuOr"
      , direction = 1
      , limits = c(0,1)
      , labels = scales::percent
    ) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8), expand = expansion(mult = c(0.1,0.2))) +
    labs(
      y = "quality"
      , x = "constrast (F-score)"
      , fill = "Pr(contrast)"
      , subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(
      legend.text = element_text(size = 7)
      , legend.title = element_text(size = 8)
    ) +
    guides(fill = guide_colorbar(theme = theme(
      legend.key.width  = unit(1, "lines"),
      legend.key.height = unit(12, "lines")
    )))

```

and summarize these contrasts

```{r}
# # can also use the following as substitute for the "tidybayes::spread_draws" used above to get same result
brms_contrast_temp %>% 
  dplyr::group_by(contrast) %>% 
  tidybayes::median_hdi(value) %>% 
  select(-c(.point,.interval, .width)) %>% 
  dplyr::arrange(desc(contrast)) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts in F-score"
    , col.names = c(
      "quality contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling()
```

Before we move on to the next section, look above at how many arguments we fiddled with to configure our `tidybayes::stat_halfeye()` plot. Given how many more contrast plots we have looming in our not-too-distant future, we might go ahead and save these settings as a new function. We'll call it `plt_contrast()`.

```{r}
plt_contrast <- function(
    my_data
    , x = "value"
    , y = "contrast"
    , fill = "pr_diff"
    , label = "pr_diff_lab"
    , label_pos = "pr_diff_lab_pos"
    , label_size = 3
    , x_expand = c(0.1, 0.1)
    , facet = NA
    , y_axis_title = ""
    , caption_text = "" # form_temp
    , annotate_size = 2.2
    ) {
  # df for annotation
  get_annotation_df <- function(
        my_text_list = c(
          "Bottom Left (h0,v0)","Top Left (h0,v1)"
          ,"Bottom Right h1,v0","Top Right h1,v1"
          )
        , hjust = c(0,0,1,1) # higher values = right, lower values = left 
        , vjust = c(0,1.3,0,1.3) # higher values = down, lower values = up
    ){
      df = data.frame(
        xpos = c(-Inf,-Inf,Inf,Inf)
        , ypos =  c(-Inf, Inf,-Inf,Inf)
        , annotate_text = my_text_list
        , hjustvar = hjust
        , vjustvar = vjust
      )  
      return(df)
  }
  # plot
  plt = 
    my_data %>%
    ggplot(aes(x = .data[[x]], y = .data[[y]])) +
      geom_vline(xintercept = 0, linetype = "solid", color = "gray33", lwd = 1.1) +
      tidybayes::stat_halfeye(
        mapping = aes(fill = .data[[fill]])
        , point_interval = median_hdi, .width = c(0.5,0.95)
        # , slab_fill = "gray22", slab_alpha = 1
        , interval_color = "black", point_color = "black", point_fill = "black"
        , point_size = 0.9
        , justification = -0.01
      ) +
      geom_text(
        data = get_annotation_df(
          my_text_list = c(
          "","L.H.S. < R.H.S."
          ,"","L.H.S. > R.H.S."
          )
        )
        , mapping = aes(
          x = xpos, y = ypos
          , hjust = hjustvar, vjust = vjustvar
          , label = annotate_text
          , fontface = "bold"
        )
        , size = annotate_size
        , color = "gray30" # "#2d2a4d" #"#204445"
      ) + 
      # scale_fill_fermenter(
      #   n.breaks = 5 # 10 use 10 if can go full range 0-1
      #   , palette = "PuOr" # "RdYlBu"
      #   , direction = 1
      #   , limits = c(0.5,1) # use c(0,1) if can go full range 0-1
      #   , labels = scales::percent
      # ) +
      scale_fill_stepsn(
        n.breaks = 5 # 10 use 10 if can go full range 0-1
        , colors = RColorBrewer::brewer.pal(11,"PuOr")[c(3,4,8,10,11)]
        , limits = c(0.5,1) # use c(0,1) if can go full range 0-1
        , labels = scales::percent
      ) +
      scale_x_continuous(expand = expansion(mult = x_expand)) +
      labs(
        y = y_axis_title
        , x = "constrast (F-score)"
        , fill = "Pr(contrast)"
        , subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI"
        , caption = caption_text
      ) +
      theme_light() +
      theme(
        legend.text = element_text(size = 7)
        , legend.title = element_text(size = 8)
        , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1.05)
        , strip.text = element_text(color = "black", face = "bold")
      ) +
      guides(fill = guide_colorbar(theme = theme(
        legend.key.width  = unit(1, "lines"),
        legend.key.height = unit(12, "lines")
      )))
  # return facet or not
  if(max(is.na(facet))==0){
    return(
      plt +
        geom_text(
          data = my_data %>%
            dplyr::filter(pr_diff_lab_pos>=0) %>% 
            dplyr::ungroup() %>% 
            dplyr::select(tidyselect::all_of(c(
              y
              , fill
              , label
              , label_pos
              , facet
            ))) %>% 
            dplyr::distinct()
          , mapping = aes(x = .data[[label_pos]], label = .data[[label]])
          , vjust = -1, hjust = 0, size = label_size
        ) +
        geom_text(
          data = my_data %>%
            dplyr::filter(pr_diff_lab_pos<0) %>% 
            dplyr::ungroup() %>% 
            dplyr::select(tidyselect::all_of(c(
              y
              , fill
              , label
              , label_pos
              , facet
            ))) %>% 
            dplyr::distinct()
          , mapping = aes(x = .data[[label_pos]], label = .data[[label]])
          , vjust = -1, hjust = +1, size = label_size
        ) +
        facet_grid(cols = vars(.data[[facet]]))
        
    )
  }
  else{
    return(
      plt +
        geom_text(
          data = my_data %>%
            dplyr::filter(pr_diff_lab_pos>=0) %>% 
            dplyr::ungroup() %>% 
            dplyr::select(tidyselect::all_of(c(
              y
              , fill
              , label
              , label_pos
            ))) %>% 
            dplyr::distinct()
          , mapping = aes(x = .data[[label_pos]], label = .data[[label]])
          , vjust = -1, hjust = 0, size = label_size
        )+
        geom_text(
          data = my_data %>%
            dplyr::filter(pr_diff_lab_pos<0) %>% 
            dplyr::ungroup() %>% 
            dplyr::select(tidyselect::all_of(c(
              y
              , fill
              , label
              , label_pos
            ))) %>% 
            dplyr::distinct()
          , mapping = aes(x = .data[[label_pos]], label = .data[[label]])
          , vjust = -1, hjust = +1, size = label_size
        )
    )
  }
}
# plt_contrast(brms_contrast_temp, label = "pr_diff_lab_sm")
```

We'll also create a function to create all of the probability labeling columns in the contrast data called `make_contrast_vars()`

Note, here we use `tidybayes::median_hdci()` to avoid potential for returning multiple rows by group if our data is grouped. See the [documentation](https://search.r-project.org/CRAN/refmans/ggdist/html/point_interval.html) for the `ggdist` package which notes that "If the distribution is multimodal, hdi may return multiple intervals for each probability level (these will be spread over rows)." 

```{r}
make_contrast_vars = function(my_data){
  my_data %>%
    dplyr::mutate(
      # get median_hdi
      median_hdi_est = tidybayes::median_hdci(value)$y
      , median_hdi_lower = tidybayes::median_hdci(value)$ymin
      , median_hdi_upper = tidybayes::median_hdci(value)$ymax
      # check probability of contrast
      , pr_gt_zero = mean(value > 0) %>% 
          scales::percent(accuracy = 1)
      , pr_lt_zero = mean(value < 0) %>% 
          scales::percent(accuracy = 1)
      # check probability that this direction is true
      , is_diff_dir = dplyr::case_when(
        median_hdi_est >= 0 ~ value > 0
        , median_hdi_est < 0 ~ value < 0
      )
      , pr_diff = mean(is_diff_dir)
      # make a label
      , pr_diff_lab = dplyr::case_when(
          median_hdi_est > 0 ~ paste0(
            "Pr("
            , stringr::word(contrast, 1, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ">"
            , stringr::word(contrast, 2, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ")="
            , pr_diff %>% scales::percent(accuracy = 1)
          )
          , median_hdi_est < 0 ~ paste0(
            "Pr("
            , stringr::word(contrast, 2, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ">"
            , stringr::word(contrast, 1, sep = fixed("-")) %>% 
              stringr::str_squish()
            , ")="
            , pr_diff %>% scales::percent(accuracy = 1)
          )
        ) %>% 
        stringr::str_replace_all("OPENDRONEMAP", "ODM") %>% 
        stringr::str_replace_all("METASHAPE", "MtaShp") %>% 
        stringr::str_replace_all("PIX4D", "Pix4D")
      # make a SMALLER label
      , pr_diff_lab_sm = dplyr::case_when(
        median_hdi_est >= 0 ~ paste0(
          "Pr(>0)="
          , pr_diff %>% scales::percent(accuracy = 1)
        )
        , median_hdi_est < 0 ~ paste0(
          "Pr(<0)="
          , pr_diff %>% scales::percent(accuracy = 1)
        )
      )
      , pr_diff_lab_pos = dplyr::case_when(
        median_hdi_est > 0 ~ median_hdi_upper
        , median_hdi_est < 0 ~ median_hdi_lower
      ) * 1.075
      , sig_level = dplyr::case_when(
        pr_diff > 0.99 ~ 0
        , pr_diff > 0.95 ~ 1
        , pr_diff > 0.9 ~ 2
        , pr_diff > 0.8 ~ 3
        , T ~ 4
      ) %>% 
      factor(levels = c(0:4), labels = c(">99%","95%","90%","80%","<80%"), ordered = T)
    )
}
# brms_contrast_temp %>% dplyr::group_by(contrast) %>% make_contrast_vars() %>% dplyr::glimpse()
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

```{r, include=FALSE, eval=FALSE}
# bayesian p-value
# http://www.stat.columbia.edu/~gelman/research/published/STS149A.pdf
# https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html

# It is the probability that a test statistic in the reference distribution exceeds its value in the data [p.value(y) = Pr(T(y.rep) > T(y))].

# Bayesian P values for mean and standard deviation test statistics The P values for the mean (P mean) give the probability that the mean of the data of new, out-of-sample observations simulated from the model exceeds the mean of the observed data. The P values for the standard deviation (P SD) give the probability that the standard deviation of new, out-of-sample observations simulated from the model exceeds the standaard deviation of the observed data. Large (> 0.90) or small (< 0.10) values indicate lack of fit.

brms::pp_check(brms_f_mod1, type = "stat_2d")
brms::pp_check(brms_f_mod1, type = "stat", stat = "sd")
brms::pp_check(brms_f_mod1, type = "stat", stat = "mean")
# vars to fit what's in the model
ptcld_validation_data %>% 
  dplyr::rename(y = f_score) %>% 
  dplyr::select(depth_maps_generation_quality, y) %>% 
  tidybayes::add_epred_draws(brms_f_mod1) %>% 
  dplyr::rename(y_rep = .epred) %>% 
  # get sd's
  dplyr::inner_join(
    brms::as_draws_df(brms_f_mod1) %>% dplyr::select(.draw,sigma)
    , by = dplyr::join_by(.draw)
  )  %>% 
  # group by draw
  dplyr::group_by(.draw) %>% 
  # make test statistic
  dplyr::summarise(
    # test statistics y
    mean_y = mean(y)
    , sd_y = sd(y)
    # test statistics y_sim
    , mean_y_rep = mean(y_rep)
    , sd_y_rep = mean(sigma)
    # , sd_y_rep = sd(y_rep)
  ) %>% 
  # summary()
  # ggplot() + geom_density(aes(x = mean_y_rep)) + geom_vline(xintercept = mean(ptcld_validation_data$f_score), lwd = 2)
  # p-values
  dplyr::ungroup() %>% 
  dplyr::mutate(
    p_val_mean = as.numeric(mean_y_rep > mean_y)
    , p_val_sd = as.numeric(sd_y_rep > sd_y)
  ) %>% 
  dplyr::select(.draw, tidyselect::starts_with("p_val_")) %>% 
  tidyr::pivot_longer(tidyselect::starts_with("p_val_"), names_prefix = "p_val_") %>% 
  dplyr::group_by(name) %>% 
  dplyr::summarise(
    mean = mean(value)
    , sd = sd(value) # not sure why this sd doesn't match what's shown in the brms::pp_check??????
    , `2.5%` = quantile(value, probs = 0.025)
    , `50%` = quantile(value, probs = 0.50)
    , `97.5%` = quantile(value, probs = 1-0.025)
  )


# the p-value should be centered on 0.5...
# ggplot()+
# geom_hist(ifelse(mean(yrep)-mean(y)>=0,1,0)) + 
# geom_vline(xintercept = 0.5, linetype = "dashed", color = "gray44") +
# geom_rect(aes(xmin = 0.9, xmax = 1, ymin = -Inf, ymax = Inf), fill = "orange3", alpha = 0.7) +
#   geom_rect(aes(xmin = 0, xmax = 0.1, ymin = -Inf, ymax = Inf), fill = "orange3", alpha = 0.7) +
#   annotate(
#     geom = "text"
#     , x = c(0,0.9)
#     , y = 0
#     , label = "lack of fit"
#     , color = "orangered4"
#     , fontface = "bold"
#     , size = 2.5
#   ) +
#   scale_y_continuous(NULL, breaks = c(NULL)) +
#   scale_x_continuous(limits = c(0,1)) 

```

## Two Nominal Predictors{#f_two_pred_mod}

Now, we'll determine the combined influence of the depth map generation quality and the depth map filtering parameters on the SfM-derived tree detection performance based on the F-score.

### Summary Statistics

Summary statistics by group:

```{r}
ptcld_validation_data %>% 
  dplyr::group_by(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
  dplyr::summarise(
    mean_f_score = mean(f_score, na.rm = T)
    # , med_f_score = median(f_score, na.rm = T)
    , sd_f_score = sd(f_score, na.rm = T)
    , n = dplyr::n()
  ) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "summary statistics: F-score by dense cloud quality and filtering mode"
    , col.names = c(
      "quality"
      , "filtering mode"
      , "mean F-score"
      , "sd"
      , "n"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```

### Bayesian{#f_two_pred_mod_bays}

[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) describes the Hierarchical Bayesian approach to describe groups of metric data with multiple nominal predictors: 

> This chapter considers data structures that consist of a metric predicted variable and two (or more) nominal predictors....The traditional treatment of this sort of data structure is called multifactor analysis of variance (ANOVA). Our Bayesian approach will be a hierarchical generalization of the traditional ANOVA model. The chapter also considers generalizations of the traditional models, because it is straight forward in Bayesian software to implement heavy-tailed distributions to accommodate outliers, along with hierarchical structure to accommodate heterogeneous variances in the different groups. (pp. 583–584)

and see section 20 from [Kurz's ebook supplement](https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-nominal-predictors.html)

The metric predicted variable with two nominal predictor variables model has the form:

\begin{align*}
y_{i}  &\sim \operatorname{Normal} \bigl(\mu_{i}, \sigma_{y} \bigr) \\
\mu_{i} &= \beta_0 + \sum_{j} \beta_{1[j]} x_{1[j]} + \sum_{k} \beta_{2[k]} x_{2[k]}  + \sum_{j,k} \beta_{1\times2[j,k]} x_{1\times2[j,k]} \\
\beta_{0}  &\sim \operatorname{Normal}(0,100) \\ 
\beta_{1[j]}  &\sim \operatorname{Normal}(0,\sigma_{\beta_{1}}) \\ 
\beta_{2[k]}  &\sim \operatorname{Normal}(0,\sigma_{\beta_{2}}) \\ 
\beta_{1\times2[j,k]}  &\sim \operatorname{Normal}(0,\sigma_{\beta_{1\times2}}) \\ 
\sigma_{\beta_{1}} &\sim \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{2}} &\sim \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{1\times2}} &\sim \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{y} &\sim {\sf Cauchy} (0,109) \\ 
\end{align*}

, where $j$ is the depth map generation quality setting corresponding to observation $i$ and $k$ is the depth map filtering mode setting corresponding to observation $i$

*for this model, we'll define the priors following [Kurz](https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-nominal-predictors.html#implementation-in-jags-brms.-1)* who notes that:

>The noise standard deviation $\sigma_y$ is depicted in the prior statement including the argument `class = sigma`...in order to be weakly informative, we will use the half-Cauchy. Recall that since the brms default is to set the lower bound for any variance parameter to 0, there’s no need to worry about doing so ourselves. So even though the syntax only indicates `cauchy`, it’s understood to mean Cauchy with a lower bound at zero; since the mean is usually 0, that makes this a half-Cauchy...The tails of the half-Cauchy are sufficiently fat that, in practice, I’ve found it doesn’t matter much what you set the $SD$ of its prior to.

```{r}
# from Kurz: 
gamma_a_b_from_omega_sigma = function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate = (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape = 1 + mode * rate
  return(list(shape = shape, rate = rate))
}

mean_y_temp = mean(ptcld_validation_data$f_score)
sd_y_temp   = sd(ptcld_validation_data$f_score)

omega_temp = sd_y_temp / 2
sigma_temp = 2 * sd_y_temp

s_r_temp = gamma_a_b_from_omega_sigma(mode = omega_temp, sd = sigma_temp)

stanvars_temp = 
  brms::stanvar(mean_y_temp,    name = "mean_y") + 
  brms::stanvar(sd_y_temp,      name = "sd_y") +
  brms::stanvar(s_r_temp$shape, name = "alpha") +
  brms::stanvar(s_r_temp$rate,  name = "beta")
```

Now fit the model.

```{r}
brms_f_mod2 = brms::brm(
  formula = f_score ~ 1 + 
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode)
  , data = ptcld_validation_data
  , family = brms::brmsfamily(family = "gaussian")
  , iter = 6000, warmup = 3000, chains = 4
  , cores = round(parallel::detectCores()/2)
  , prior = c(
    brms::prior(normal(mean_y, sd_y * 5), class = "Intercept")
    , brms::prior(gamma(alpha, beta), class = "sd")
    , brms::prior(cauchy(0, sd_y), class = "sigma")
  )
  , stanvars = stanvars_temp
  , file = paste0(rootdir, "/fits/brms_f_mod2")
)
```

check the trace plots for problems with convergence of the Markov chains

```{r, fig.height=8}
plot(brms_f_mod2)
```

check the prior distributions

```{r}
# check priors
brms::prior_summary(brms_f_mod2) %>% 
  kableExtra::kbl() %>% 
  kableExtra::kable_styling()
```

The `brms::brm` model summary

```{r}
brms_f_mod2 %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | stringr::str_starts(parameter, "sd_") 
    | parameter == "sigma"
  ) %>%
  dplyr::mutate(
    parameter = parameter %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
  ) %>% 
  kableExtra::kbl(digits = 2, caption = "Bayesian two nominal predictors: F-score by dense cloud quality and filtering mode") %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```


We can look at the model noise standard deviation $\sigma_y$

```{r}
# get formula
form_temp = brms_f_mod2$formula$formula[3] %>% 
  as.character() %>% get_frmla_text() %>% 
  stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
  stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
# extract the posterior draws
brms::as_draws_df(brms_f_mod2) %>% 
# plot
  ggplot(aes(x = sigma, y = 0)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21, point_size = 3
    , quantiles = 100
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\sigma_y$")
    , caption = form_temp
  ) +
  theme_light()
```

how is it compared to the first model?

```{r}
# how is it compared to the first model
dplyr::bind_rows(
    brms::as_draws_df(brms_f_mod1) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "one nominal predictor")
    , brms::as_draws_df(brms_f_mod2) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "two nominal predictor")
  ) %>% 
  dplyr::relocate(model) %>% 
  kableExtra::kbl(digits = 2, caption = "brms::brm model noise standard deviation comparison") %>% 
    kableExtra::kable_styling()
```

plot the posterior predictive distributions of the conditional means with the median F-score and the 95% highest posterior density interval (HDI)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(brms_f_mod2, allow_new_levels = T) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_eye(
      point_interval = median_hdi, .width = .95
      , slab_alpha = 0.8
      , interval_color = "black", linewidth = 1
      , point_color = "black", point_fill = "black", point_size = 1
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
    facet_grid(cols = vars(depth_maps_generation_quality)) +
    labs(
      x = "filtering mode", y = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
      , fill = "Filtering Mode"
      , caption = form_temp
    ) +
    theme_light() +
    theme(
      legend.position = "none"
      , legend.direction  = "horizontal"
      , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
      , strip.text = element_text(color = "black", face = "bold")
    ) 
```

we can also make pairwise comparisons

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
    tidybayes::add_epred_draws(brms_f_mod2, allow_new_levels = T) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = 
        contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
    , depth_maps_generation_filtering_mode = depth_maps_generation_filtering_mode %>% 
      factor(
        levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
        , ordered = T
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, depth_maps_generation_filtering_mode) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "depth_maps_generation_filtering_mode"
  , label_size = 2.1
  , x_expand = c(0,0.65)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby filtering mode"
  )
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_filtering_mode) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_filtering_mode) %>% 
  dplyr::select(contrast, depth_maps_generation_filtering_mode, value, .lower, .upper) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "filtering mode"
      , "difference (F-score)"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```


[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) notes that for the multiple nominal predictors model: 

> In applications with multiple levels of the factors, it is virtually always the case that we are interested in comparing particular levels with each other...These sorts of comparisons, which involve levels of a single factor and collapse across the other factor(s), are called main effect comparisons or contrasts.(p. 595)

First, let's collapse across the filtering mode to compare the dense cloud quality setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod2
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_quality
      , fill = depth_maps_generation_quality
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "inferno", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "quality", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [one nominal predictor model above](#f_one_pred_mod_bays)

```{r}
# let's compare these results to the results from our [one nominal predictor model above](#one_pred_mod)
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod2
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod2") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(brms_f_mod1) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod1")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(depth_maps_generation_quality), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[1:2]) +
  labs(
    y = "", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

these results are as expected, with [Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) noting:

> It is important to realize that the estimates of interaction contrasts are typically much more uncertain than the estimates of simple effects or main effects...This large uncertainty of an interaction contrast is caused by the fact that it involves at least four sources of uncertainty (i.e., at least four groups of data), unlike its component simple effects which each involve only half of those sources of uncertainty. In general, interaction contrasts require a lot of data to estimate accurately. (p. 598)

For completeness, let's also collapse across the dense cloud quality to compare the filtering mode setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod2
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "filtering mode", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

...it looks like the variation in F-score is driven by the dense cloud quality setting. We can quantify the variation in F-score by comparing the $\sigma$ posteriors.

```{r}
# extract the posterior draws
brms::as_draws_df(brms_f_mod2) %>% 
  dplyr::select(c(sigma,tidyselect::starts_with("sd_"))) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  # dplyr::group_by(name) %>% 
  # tidybayes::median_hdi(value) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering") %>% 
      forcats::fct_reorder(value)
  ) %>%
# plot
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21 #, point_size = 3
    , quantiles = 100
  ) +
  labs(x = "", y = "", caption = form_temp) +
  theme_light()
```

Finally we can perform model selection via information criteria, from section 10 in [Kurz's ebook supplement](https://bookdown.org/content/3686/metric-predicted-variable-with-one-nominal-predictor.html):

> expected log predictive density (`elpd_loo`), the estimated effective number of parameters (`p_loo`), and the Pareto smoothed importance-sampling leave-one-out cross-validation (PSIS-LOO; `looic`). Each estimate comes with a standard error (i.e., `SE`). Like other information criteria, the LOO values aren't of interest in and of themselves. However, the estimate of one model's LOO relative to that of another can be of great interest. We generally prefer models with lower information criteria. With the `brms::loo_compare()` function, we can compute a formal difference score between two models...The `brms::loo_compare()` output rank orders the models such that the best fitting model appears on top.

```{r}
brms_f_mod1 = brms::add_criterion(brms_f_mod1, criterion = c("loo", "waic"))
brms_f_mod2 = brms::add_criterion(brms_f_mod2, criterion = c("loo", "waic"))
brms::loo_compare(brms_f_mod1, brms_f_mod2, criterion = "loo")
# brms::model_weights(brms_f_mod1, brms_f_mod2) %>% round()
```

These models are not significantly different suggesting that the filtering mode is not contributing much to the variation in SfM-derived tree detection reliability after accounting for the quality setting

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Three Nominal Predictors{#f_three_pred_mod}

Now, we'll add the SfM processing software to our model which includes the depth map generation quality and the depth map filtering parameters to quantify the SfM-derived tree detection performance based on the F-score.

### Summary Statistics

Summary statistics by group:

```{r}
ptcld_validation_data %>% 
  dplyr::group_by(depth_maps_generation_quality, depth_maps_generation_filtering_mode, software) %>%
  dplyr::summarise(
    mean_f_score = mean(f_score, na.rm = T)
    # , med_f_score = median(f_score, na.rm = T)
    , sd_f_score = sd(f_score, na.rm = T)
    , n = dplyr::n()
  ) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "summary statistics: F-score by dense cloud quality, filtering mode, and software"
    , col.names = c(
      "quality"
      , "filtering mode"
      , "software"
      , "mean F-score"
      , "sd"
      , "n"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "8in")
```

we can view this data by using `ggplot2::geom_tile`

```{r, fig.height=8}
ptcld_validation_data %>% 
  dplyr::group_by(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>%
  # collapse across study site
  dplyr::summarise(
    mean_f_score = mean(f_score, na.rm = T)
    # , med_f_score = median(f_score, na.rm = T)
    , sd_f_score = sd(f_score, na.rm = T)
    , n = dplyr::n()
  ) %>% 
  ggplot(mapping = aes(
    y = depth_maps_generation_quality
    , x = depth_maps_generation_filtering_mode
    , fill = mean_f_score
    , label = paste0(scales::comma(mean_f_score,accuracy = 0.01), "\n(n=", n,")")
  )) +
  geom_tile(color = "white") +
  geom_text(color = "white", size = 3) +
  facet_grid(cols = vars(software)) + 
  scale_x_discrete(expand = c(0, 0)) +
  scale_y_discrete(expand = c(0, 0)) +
  scale_fill_viridis_c(option = "cividis", begin = 0.3, end = 0.9) +
  labs(
    x = "filtering mode"
    , y = "quality"
    , fill = "F-score"
    , subtitle = "mean F-score and # of study sites"
  ) +
  theme_light() + 
  theme(
    legend.position = "none"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , panel.background = element_blank()
    , panel.grid = element_blank()
    , plot.subtitle = element_text(hjust = 0.5)
    , strip.text = element_text(color = "black", face = "bold")
  )

ggplot2::ggsave("../data/fscore_comp_quick.png", height = 9, width = 8)
```


### Bayesian{#f_three_pred_mod_bays}

[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) describes the Hierarchical Bayesian approach to extend our two nominal predictor model to include another nominal predictor (referred to as "subject" here but the methodology applies for other nominal variables): 

> When every subject contributes many measurements to every cell, then the model of the situation is a straight-forward extension of the models we have already considered. We merely add "subject" as another nominal predictor in the model, with each individual subject being a level of the predictor. If there is one predictor other than subject, the model becomes
>
> $$ y = \beta_0 + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_S \overrightarrow x_S + \overrightarrow \beta_{1 \times S} \overrightarrow x_{1 \times S} $$
>
> This is exactly the two-predictor model we have already considered, with the second predictor being subject. When there are two predictors other than subject, the model becomes
> 
> \begin{align*}
> y = & \; \beta_0 & \text{baseline} \\
> & + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_2 \overrightarrow x_2 + \overrightarrow \beta_S \overrightarrow x_S  & \text{main effects} \\
> & + \overrightarrow \beta_{1 \times 2} \overrightarrow x_{1 \times 2} + \overrightarrow \beta_{1 \times S} \overrightarrow x_{1 \times S} + \overrightarrow \beta_{2 \times S} \overrightarrow x_{2 \times S} & \text{two-way interactions} \\
> & + \overrightarrow \beta_{1 \times 2 \times S} \overrightarrow x_{1 \times 2 \times S} & \text{three-way interactions}
> \end{align*}
> 
> This model includes all the two-way interactions of the factors, plus the three-way interaction. (p. 607)

The metric predicted variable with three nominal predictor variables model has the form:

\begin{align*}
y_{i} \sim & \operatorname{Normal} \bigl(\mu_{i}, \sigma_{y} \bigr) \\
\mu_{i} = & \beta_0 \\
& + \sum_{j} \beta_{1[j]} x_{1[j]} + \sum_{k} \beta_{2[k]} x_{2[k]} + \sum_{f} \beta_{3[f]} x_{3[f]}  \\
& + \sum_{j,k} \beta_{1\times2[j,k]} x_{1\times2[j,k]} + \sum_{j,f} \beta_{1\times3[j,f]} x_{1\times3[j,f]} + \sum_{k,f} \beta_{2\times3[k,f]} x_{2\times3[k,f]} \\
& + \sum_{j,k,f} \beta_{1\times2\times3[j,k,f]} x_{1\times2\times3[j,k,f]} \\
\beta_{0}  \sim & \operatorname{Normal}(0,100) \\ 
\beta_{1[j]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1}}) \\ 
\beta_{2[k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2}}) \\ 
\beta_{3[f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{3}}) \\ 
\beta_{1\times2[j,k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times2}}) \\ 
\beta_{1\times3[j,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times3}}) \\ 
\beta_{2\times3[k,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2\times3}}) \\ 
\sigma_{\beta_{1}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{2}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{1\times2}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{1\times3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{2\times3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{y} \sim & {\sf Cauchy} (0,109) \\ 
\end{align*}

, where $j$ is the depth map generation quality setting corresponding to observation $i$, $k$ is the depth map filtering mode setting corresponding to observation $i$, and $f$ is the processing software corresponding to observation $i$

*for this model, we'll define the priors following [Kurz](https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-nominal-predictors.html#implementation-in-jags-brms.-1)* who notes that:

>The noise standard deviation $\sigma_y$ is depicted in the prior statement including the argument `class = sigma`...in order to be weakly informative, we will use the half-Cauchy. Recall that since the brms default is to set the lower bound for any variance parameter to 0, there’s no need to worry about doing so ourselves. So even though the syntax only indicates `cauchy`, it’s understood to mean Cauchy with a lower bound at zero; since the mean is usually 0, that makes this a half-Cauchy...The tails of the half-Cauchy are sufficiently fat that, in practice, I’ve found it doesn’t matter much what you set the $SD$ of its prior to.

```{r}
# from Kurz: 
gamma_a_b_from_omega_sigma = function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate = (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape = 1 + mode * rate
  return(list(shape = shape, rate = rate))
}

mean_y_temp = mean(ptcld_validation_data$f_score)
sd_y_temp   = sd(ptcld_validation_data$f_score)

omega_temp = sd_y_temp / 2
sigma_temp = 2 * sd_y_temp

s_r_temp = gamma_a_b_from_omega_sigma(mode = omega_temp, sd = sigma_temp)

stanvars_temp = 
  brms::stanvar(mean_y_temp,    name = "mean_y") + 
  brms::stanvar(sd_y_temp,      name = "sd_y") +
  brms::stanvar(s_r_temp$shape, name = "alpha") +
  brms::stanvar(s_r_temp$rate,  name = "beta")
```

Now fit the model.

```{r}
brms_f_mod3 = brms::brm(
  formula = f_score ~ 
    # baseline
    1 + 
    # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
  , data = ptcld_validation_data
  , family = brms::brmsfamily(family = "gaussian")
  , iter = 20000, warmup = 10000, chains = 4
  , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = round(parallel::detectCores()/2)
  , prior = c(
    brms::prior(normal(mean_y, sd_y * 5), class = "Intercept")
    , brms::prior(gamma(alpha, beta), class = "sd")
    , brms::prior(cauchy(0, sd_y), class = "sigma")
  )
  , stanvars = stanvars_temp
  , file = paste0(rootdir, "/fits/brms_f_mod3")
)
# https://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
# https://mc-stan.org/misc/warnings.html#bulk-ess
# https://mc-stan.org/misc/warnings.html#tail-ess 
```

check the trace plots for problems with convergence of the Markov chains

```{r, fig.height=8}
plot(brms_f_mod3)
```

check the prior distributions

```{r}
# check priors
brms::prior_summary(brms_f_mod3) %>% 
  kableExtra::kbl() %>% 
  kableExtra::kable_styling()
```

The `brms::brm` model summary

We won't clutter the output here but this can be run if you are following along on your own

```{r, eval=F}
brms_f_mod3 %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | stringr::str_starts(parameter, "sd_") 
    | parameter == "sigma"
  ) %>%
  dplyr::mutate(
    parameter = parameter %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
  ) %>% 
  kableExtra::kbl(digits = 2, caption = "Bayesian 3 nominal predictors for F-score") %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "8in")
```

We can look at the model noise standard deviation $\sigma_y$

```{r}
# get formula
form_temp = brms_f_mod3$formula$formula[3] %>% 
  as.character() %>% get_frmla_text() %>% 
  stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
  stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
# extract the posterior draws
brms::as_draws_df(brms_f_mod3) %>% 
# plot
  ggplot(aes(x = sigma, y = 0)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21, point_size = 3
    , quantiles = 100
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\sigma_y$")
    , caption = form_temp
  ) +
  theme_light()
```

how is it compared to our other models?

```{r}
# how is it compared to our other models?
dplyr::bind_rows(
    brms::as_draws_df(brms_f_mod1) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "one nominal predictor")
    , brms::as_draws_df(brms_f_mod2) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "two nominal predictor")
    , brms::as_draws_df(brms_f_mod3) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "three nominal predictor")
  ) %>% 
  dplyr::relocate(model) %>% 
  kableExtra::kbl(digits = 2, caption = "brms::brm model noise standard deviation comparison") %>% 
    kableExtra::kable_styling()
```

plot the posterior predictive distributions of the conditional means with the median F-score and the 95% highest posterior density interval (HDI)

Note that how within `tidybayes::add_epred_draws`, we used the `re_formula` argument to average over the random effects of `software`. For this model we have to collapse across the software effects to compare the dense cloud quality and filtering mode setting effects.

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod3, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | depth_maps_generation_filtering_mode) + 
      (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_eye(
      point_interval = median_hdi, .width = .95
      , slab_alpha = 0.8
      , interval_color = "black", linewidth = 1
      , point_color = "black", point_fill = "black", point_size = 1
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
    facet_grid(cols = vars(depth_maps_generation_quality)) +
    labs(
      x = "filtering mode", y = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
      , fill = "Filtering Mode"
      , caption = form_temp
    ) +
    theme_light() +
    theme(
      legend.position = "none"
      , legend.direction  = "horizontal"
      , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
      , strip.text = element_text(color = "black", face = "bold")
    ) 
    
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod3, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_quality) +
        (1 | depth_maps_generation_filtering_mode) + 
        (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = 
        contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
    , depth_maps_generation_filtering_mode = depth_maps_generation_filtering_mode %>% 
      factor(
        levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
        , ordered = T
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, depth_maps_generation_filtering_mode) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "depth_maps_generation_filtering_mode"
  , label_size = 2.1
  , x_expand = c(0,0.6)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby filtering mode"
  )

```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_filtering_mode) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_filtering_mode) %>% 
  dplyr::select(contrast, depth_maps_generation_filtering_mode, value, .lower, .upper) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "filtering mode"
      , "difference (F-score)"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```

It might be more important to understand the difference in F-score by dense cloud quality and software rather than filtering mode since filtering mode had such a small effect on the SfM predictive ability

Note that how within `tidybayes::add_epred_draws`, we used the `re_formula` argument to average over the random effects of `depth_maps_generation_filtering_mode`. For this model we have to collapse across the filtering mode effects to compare the dense cloud quality and software setting effects.

```{r}
qlty_sftwr_draws_temp = ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality, software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod3, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | software) + 
      (1 | depth_maps_generation_quality:software)
  ) %>% 
  dplyr::rename(value = .epred) 
# plot it
qlty_sftwr_draws_temp %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = software
      , fill = software
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.8
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
  scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
  facet_grid(cols = vars(depth_maps_generation_quality)) +
  labs(
    x = "software", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
    , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
  ) 
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
brms_contrast_temp = qlty_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_quality
    , comparison = 
      contrast_list[!stringr::str_detect(contrast_list, "lowest")]
      # contrast_list
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "software"
  , label_size = 2.2
  , x_expand = c(0,0.5)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby software"
  )

```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, software) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, software) %>% 
  dplyr::select(contrast, software, value, .lower, .upper) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "software"
      , "difference (F-score)"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```

let's collapse across the filtering mode and software to compare the dense cloud quality setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod3
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_quality
      , fill = depth_maps_generation_quality
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "inferno", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "quality", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [one nominal predictor model above](#f_one_pred_mod_bays) and [two nominal predictor model](#f_two_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod3
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod3") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod2
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_quality)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod2")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(brms_f_mod1) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod1")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(depth_maps_generation_quality), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[1:3]) +
  labs(
    y = "", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

let's also collapse across the dense cloud quality and software to compare the filtering mode setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod3
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "filtering mode", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [two nominal predictor model above](#f_two_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod3
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod3") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod2
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod2")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(depth_maps_generation_filtering_mode), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[2:3]) +
  labs(
    y = "filtering mode", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

to address one of our main questions, let's also collapse across the dense cloud quality and filtering mode setting to compare the software effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod3
    # this part is crucial
    , re_formula = ~ (1 | software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = software
      , fill = software
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "software", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

Finally, we can quantify the variation in F-score by comparing the $\sigma$ posteriors.

```{r}
# extract the posterior draws
brms::as_draws_df(brms_f_mod3) %>% 
  dplyr::select(c(sigma,tidyselect::starts_with("sd_"))) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  # dplyr::group_by(name) %>% 
  # tidybayes::median_hdi(value) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering") %>% 
      forcats::fct_reorder(value)
  ) %>%
# plot
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21 #, point_size = 3
    , quantiles = 100
  ) +
  labs(x = "", y = "", caption = form_temp) +
  theme_light()
```

and perform model selection via information criteria with the `brms::loo_compare()` function

```{r}
brms_f_mod3 = brms::add_criterion(brms_f_mod3, criterion = c("loo", "waic"))
brms::loo_compare(brms_f_mod1, brms_f_mod2, brms_f_mod3, criterion = "loo")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Three Nominal Predictors + site effects

Now, we'll add the average deflection from the baseline (i.e. the "grand mean") due to study site (i.e. the "subjects" in our data). The main effect for the study site will be added to our model with the combined influence of the depth map generation quality, the depth map filtering, and the software on the F-score.

In the model we use below, the study site is modeled as a "random effect." [Hobbs et al. (2024)](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1598) describe a similar model:

>It is important to understand that fitting treatment intercepts and slopes as random rather than fixed means that our inference applied to all possible sites suitable for [inclusion in the study]. In contrast, assuming fixed effects of treatment would dramatically reduce the uncertainty about those effects, but would constrain inference to the four sites that we studied. (p. 13)

### Summary Statistics

Each study site contributes one observation per dense cloud quality, filtering mode, and software. That is, a row in the underlying data is unique by study site, software, dense cloud quality, and filtering mode.

```{r}
identical(
  # base data
  nrow(ptcld_validation_data)
  # distinct group
  , ptcld_validation_data %>% 
    dplyr::distinct(
      study_site
      , software
      , depth_maps_generation_quality
      , depth_maps_generation_filtering_mode
    ) %>% 
    nrow()
)
```

### Bayesian{#f_4_pred_mod_bays}

[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) describes the Hierarchical Bayesian approach to describe groups of metric data with multiple nominal predictors when every subject ("study site" in our research) only contributes one observation per cell/condition: 

> \begin{align*}
> y = & \; \beta_0 \\
> & + \overrightarrow \beta_1 \overrightarrow x_1 + \overrightarrow \beta_2 \overrightarrow x_2 + \overrightarrow \beta_{1 \times 2} \overrightarrow x_{1 \times 2} \\
> & + \overrightarrow \beta_S \overrightarrow x_S
> \end{align*}

> In other words, we assume a main effect of subject, but no interaction of subject with other predictors. In this model, the subject effect (deflection) is constant across treatments, and the treatment effects (deflections) are constant across subjects. Notice that the model makes no requirement that every subject contributes a datum to every condition. Indeed, the model allows zero or multiple data per subject per condition. Bayesian estimation makes no assumptions or requirements that the design is balanced (i.e., has equal numbers of measurement in each cell). (p. 608)

and see section 20 from [Kurz's ebook supplement](https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-nominal-predictors.html)

The metric predicted variable with three nominal predictor variables and subject-level effects model has the form:

\begin{align*}
y_{i} \sim & \operatorname{Normal} \bigl(\mu_{i}, \sigma_{y} \bigr) \\
\mu_{i} = & \beta_0 \\
& + \sum_{j} \beta_{1[j]} x_{1[j]} + \sum_{k} \beta_{2[k]} x_{2[k]} + \sum_{f} \beta_{3[f]} x_{3[f]} + \sum_{s} \beta_{4[s]} x_{4[s]}  \\
& + \sum_{j,k} \beta_{1\times2[j,k]} x_{1\times2[j,k]} + \sum_{j,f} \beta_{1\times3[j,f]} x_{1\times3[j,f]} + \sum_{k,f} \beta_{2\times3[k,f]} x_{2\times3[k,f]} \\
& + \sum_{j,k,f} \beta_{1\times2\times3[j,k,f]} x_{1\times2\times3[j,k,f]} \\
\beta_{0}  \sim & \operatorname{Normal}(0,100) \\ 
\beta_{1[j]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1}}) \\ 
\beta_{2[k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2}}) \\ 
\beta_{3[f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{3}}) \\ 
\beta_{4[s]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{4}}) \\ 
\beta_{1\times2[j,k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times2}}) \\ 
\beta_{1\times3[j,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times3}}) \\ 
\beta_{2\times3[k,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2\times3}}) \\ 
\sigma_{\beta_{1}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{2}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{4}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{1\times2}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{1\times3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{2\times3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{y} \sim & {\sf Cauchy} (0,109) \\ 
\end{align*}

, where $j$ is the depth map generation quality setting corresponding to observation $i$, $k$ is the depth map filtering mode setting corresponding to observation $i$, $f$ is the processing software corresponding to observation $i$, and $s$ is the study site corresponding to observation $i$

*for this model, we'll define the priors following [Kurz](https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-nominal-predictors.html#implementation-in-jags-brms.-1)*:

```{r}
# from Kurz: 
gamma_a_b_from_omega_sigma = function(mode, sd) {
  if (mode <= 0) stop("mode must be > 0")
  if (sd   <= 0) stop("sd must be > 0")
  rate = (mode + sqrt(mode^2 + 4 * sd^2)) / (2 * sd^2)
  shape = 1 + mode * rate
  return(list(shape = shape, rate = rate))
}

mean_y_temp = mean(ptcld_validation_data$f_score)
sd_y_temp   = sd(ptcld_validation_data$f_score)

omega_temp = sd_y_temp / 2
sigma_temp = 2 * sd_y_temp

s_r_temp = gamma_a_b_from_omega_sigma(mode = omega_temp, sd = sigma_temp)

stanvars_temp = 
  brms::stanvar(mean_y_temp,    name = "mean_y") + 
  brms::stanvar(sd_y_temp,      name = "sd_y") +
  brms::stanvar(s_r_temp$shape, name = "alpha") +
  brms::stanvar(s_r_temp$rate,  name = "beta")
```

Now fit the model.

```{r}
brms_f_mod4 = brms::brm(
  formula = f_score ~ 
    # baseline
    1 + 
    # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    (1 | study_site) + # only fitting main effects of site and not interactions
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
  , data = ptcld_validation_data
  , family = brms::brmsfamily(family = "gaussian")
  , iter = 20000, warmup = 10000, chains = 4
  , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = round(parallel::detectCores()/2)
  , prior = c(
    brms::prior(normal(mean_y, sd_y * 5), class = "Intercept")
    , brms::prior(gamma(alpha, beta), class = "sd")
    , brms::prior(cauchy(0, sd_y), class = "sigma")
  )
  , stanvars = stanvars_temp
  , file = paste0(rootdir, "/fits/brms_f_mod4")
)
```

check the trace plots for problems with convergence of the Markov chains

```{r, fig.height=8}
plot(brms_f_mod4)
```

check the prior distributions

```{r}
# check priors
brms::prior_summary(brms_f_mod4) %>% 
  kableExtra::kbl() %>% 
  kableExtra::kable_styling()
```

The `brms::brm` model summary

We won't clutter the output here but this can be run if you are following along on your own

```{r, eval=F}
brms_f_mod4 %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | stringr::str_starts(parameter, "sd_") 
    | parameter == "sigma"
  ) %>%
  dplyr::mutate(
    parameter = parameter %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
  ) %>% 
  kableExtra::kbl(digits = 2, caption = "Bayesian 3 nominal predictors + study site effects for F-score") %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```

We can look at the model noise standard deviation $\sigma_y$

```{r}
# get formula
form_temp = brms_f_mod4$formula$formula[3] %>% 
  as.character() %>% get_frmla_text() %>% 
  stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
  stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
# extract the posterior draws
brms::as_draws_df(brms_f_mod4) %>% 
# plot
  ggplot(aes(x = sigma, y = 0)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21, point_size = 3
    , quantiles = 100
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\sigma_y$")
    , caption = form_temp
  ) +
  theme_light()
```

how is it compared to our other models?

```{r}
# how is it compared to our other models?
dplyr::bind_rows(
    brms::as_draws_df(brms_f_mod1) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "one nominal predictor")
    , brms::as_draws_df(brms_f_mod2) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "two nominal predictor")
    , brms::as_draws_df(brms_f_mod3) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "three nominal predictor")
    , brms::as_draws_df(brms_f_mod4) %>% tidybayes::median_hdi(sigma) %>% dplyr::mutate(model = "three+site nominal predictor")
  ) %>% 
  dplyr::relocate(model) %>% 
  kableExtra::kbl(digits = 2, caption = "brms::brm model noise standard deviation comparison") %>% 
    kableExtra::kable_styling()
```

plot the posterior predictive distributions of the conditional means with the median F-score and the 95% highest posterior density interval (HDI)

Note that how within `tidybayes::add_epred_draws`, we used the `re_formula` argument to average over the random effects of `study_site` (i.e., we left `(1 | study_site)` out of the formula) and `software` (i.e., we left `(1 | software)` and its interactions out of the formula).

```{r}
qlty_filter_draws_temp = 
  ptcld_validation_data %>% 
    dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod4, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_quality) +
        (1 | depth_maps_generation_filtering_mode) + 
        (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode)
    ) %>% 
    dplyr::rename(value = .epred) 
# hey plot
qlty_filter_draws_temp %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_eye(
      point_interval = median_hdi, .width = .95
      , slab_alpha = 0.8
      , interval_color = "black", linewidth = 1
      , point_color = "black", point_fill = "black", point_size = 1
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
    facet_grid(cols = vars(depth_maps_generation_quality)) +
    labs(
      x = "filtering mode", y = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
      , fill = "Filtering Mode"
      , caption = form_temp
    ) +
    theme_light() +
    theme(
      legend.position = "none"
      , legend.direction  = "horizontal"
      , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
      , strip.text = element_text(color = "black", face = "bold")
    ) 
    
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
brms_contrast_temp = qlty_filter_draws_temp %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = 
        contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
    , depth_maps_generation_filtering_mode = depth_maps_generation_filtering_mode %>% 
      factor(
        levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
        , ordered = T
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast,depth_maps_generation_filtering_mode) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "depth_maps_generation_filtering_mode"
  , label_size = 2.2
  , x_expand = c(-0.1,0.43)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby filtering mode"
  )

```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_filtering_mode) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_filtering_mode) %>% 
  dplyr::select(contrast, depth_maps_generation_filtering_mode, value, .lower, .upper) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "filtering mode"
      , "difference (F-score)"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "8in")
```

It might be more important to understand the difference in F-score by dense cloud quality and software rather than filtering mode since filtering mode had such a small effect on the SfM predictive ability

Note that how within `tidybayes::add_epred_draws`, we used the `re_formula` argument to average over the random effects of `depth_maps_generation_filtering_mode`. For this model we have to collapse across the filtering mode effects to compare the dense cloud quality and software setting effects.

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality, software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | software) + 
      (1 | depth_maps_generation_quality:software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = software
      , fill = software
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.8
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
  scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
  facet_grid(cols = vars(depth_maps_generation_quality)) +
  labs(
    x = "software", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
    , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
  ) 
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
# get draws
qlty_sftwr_draws_temp = ptcld_validation_data %>%
  dplyr::distinct(depth_maps_generation_quality, software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | software) + 
      (1 | depth_maps_generation_quality:software)
  ) %>% 
  dplyr::rename(value = .epred)

# calculate contrast
brms_contrast_temp = qlty_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_quality
    , comparison = 
      contrast_list[!stringr::str_detect(contrast_list, "lowest")]
      # contrast_list
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "software"
  , label_size = 2.1
  , x_expand = c(0,0.5)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby software"
  )

ggplot2::ggsave(
  "../data/qlty_sftwr_comp_mod4.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, software) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, software) %>% 
  dplyr::select(contrast, software, value, .lower, .upper) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "software"
      , "difference (F-score)"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```

The contrasts above address the question "are there differences in F-score based on dense point cloud generation quality within each software?".

To address the different question of "are there differences in F-score based on the processing software used at a given dense point cloud generation quality?" we need to utilize a different formulation of the `comparison` parameter within our call to the `tidybayes::compare_levels` function and calculate the contrast by `software` instead

```{r}
# calculate contrast
brms_contrast_temp = qlty_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = software
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = software) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, depth_maps_generation_quality) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "software"
  , facet = "depth_maps_generation_quality"
  , label_size = 2.0
  , x_expand = c(0.3,0.3)
) +
  facet_wrap(facets = vars(depth_maps_generation_quality), ncol = 2) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby dense cloud quality"
  ) +
  theme(
    legend.position = c(.75, .13)
  ) +
  guides(fill = guide_colorbar(theme = theme(
    legend.key.width  = unit(1, "lines"),
    legend.key.height = unit(7, "lines")
  )))
```


```{r, include=FALSE, eval=FALSE}
# plot it
brms_contrast_temp %>% 
  ggplot(
    mapping = aes(
      x = value, y = contrast
      , fill = pr_diff
    )
  ) +
  tidybayes::stat_halfeye(
    point_interval = median_hdi, .width = c(0.5,0.95)
    # , slab_fill = "gray22", slab_alpha = 1
    , interval_color = "black", point_color = "black", point_fill = "black"
    , justification = -0.01
  ) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray44") +
  scale_fill_fermenter(
    n.breaks = 10, palette = "PuOr"
    , direction = 1
    , limits = c(0,1)
    , labels = scales::percent
  ) +
  # scale_fill_viridis_c(
  #   option = "turbo", begin = 0.3, direction = -1
  #   , limits = c(0,1)
  #   , breaks = scales::extended_breaks(n=6)
  #   , labels = scales::percent
  # ) +
  scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
  facet_wrap(facets = vars(depth_maps_generation_quality), ncol = 2) +
  labs(
    y = "software"
    , x = "constrast (F-score)"
    , subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby dense cloud quality"
    , fill = "Pr(abs(contrast) > 0.05)"
    , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = c(.75, .13)
    , legend.text = element_text(size = 7)
    , legend.title = element_text(size = 8)
    , strip.text = element_text(color = "black", face = "bold")
  ) +
  guides(fill = guide_colorbar(theme = theme(
    legend.key.width  = unit(1, "lines"),
    legend.key.height = unit(7, "lines")
  )))

ggplot2::ggsave(
  "../data/sftwr_qlty_comp_mod4.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 8, width = 6
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_quality) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_quality) %>% 
  dplyr::select(contrast, depth_maps_generation_quality, value, .lower, .upper) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "software contrast"
      , "quality"
      , "difference (F-score)"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "8in")
```

let's collapse across the filtering mode, software, and study site to compare the dense cloud quality setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_quality
      , fill = depth_maps_generation_quality
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "inferno", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "quality", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [one nominal predictor model above](#f_one_pred_mod_bays), [two nominal predictor model above](#f_two_pred_mod_bays), and [three nominal predictor model above](#f_three_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod4") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod3
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_quality)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod3")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod2
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_quality)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod2")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(brms_f_mod1) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod1")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(depth_maps_generation_quality), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[1:4]) +
  labs(
    y = "", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

let's also collapse across the dense cloud quality, software, and study site to compare the filtering mode setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "filtering mode", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from [two nominal predictor model above](#f_two_pred_mod_bays) and [three nominal predictor model above](#f_three_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod4") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod3
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod3")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod2
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod2")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(depth_maps_generation_filtering_mode), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[2:4]) +
  labs(
    y = "filtering mode", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

to address one of our main questions, let's also collapse across the study site, dense cloud quality, and filtering mode setting to compare the software effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4
    # this part is crucial
    , re_formula = ~ (1 | software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = software
      , fill = software
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "software", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [three nominal predictor model above](#f_three_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod4
    # this part is crucial
    , re_formula = ~ (1 | software)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod4") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(software) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod3
        # this part is crucial
        , re_formula = ~ (1 | software)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod3")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(software), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[3:4]) +
  labs(
    y = "", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

Finally, we can quantify the variation in F-score by comparing the $\sigma$ posteriors.

```{r}
# extract the posterior draws
brms::as_draws_df(brms_f_mod4) %>% 
  dplyr::select(c(sigma,tidyselect::starts_with("sd_"))) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  # dplyr::group_by(name) %>% 
  # tidybayes::median_hdi(value) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering") %>% 
      forcats::fct_reorder(value)
  ) %>%
# plot
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21 #, point_size = 3
    , quantiles = 100
  ) +
  labs(x = "", y = "", caption = form_temp) +
  theme_light()
```

and perform model selection via information criteria with the `brms::loo_compare()` function

```{r}
brms_f_mod4 = brms::add_criterion(brms_f_mod4, criterion = c("loo", "waic"))
brms::loo_compare(brms_f_mod1, brms_f_mod2, brms_f_mod3, brms_f_mod4, criterion = "loo")
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```


## The beta: Three Nominal Predictors + site effects{#beta_mod}

To this point, we have been modelling F-score presuming a Gaussian likelihood. However, the beta likelihood more accurately represents the F-score data which is continuous and restricted within the range of $(0,1)$.

We borrow here from the excellent series on causal inference by [A. Solomon Kurz](https://solomonkurz.netlify.app/blog/2023-06-25-causal-inference-with-beta-regression/). We also utilize the guide to Bayesian beta models by [Andrew Heiss](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#b-beta-regression-bayesian-style) while [Nicole Knight](https://rpubs.com/nicoleknight/936037) posted about the Beta for ecological data.

### Summary Statistics

let's check our underlying data for F-score (our dependent or $y$ variable)

```{r}
# distribution
ptcld_validation_data %>% 
  ggplot(mapping = aes(x = f_score)) + 
  geom_hline(yintercept = 0) +
  geom_vline(xintercept = c(0,1)) +
  geom_density(fill = "lightblue", alpha = 0.7, color = NA) +
  labs(y="",x="F-score") +
  scale_y_continuous(breaks = c(0)) +
  scale_x_continuous(breaks = scales::extended_breaks(10)) +
  theme_light() +
  theme(panel.grid = element_blank())
```

and the summary statistics

```{r}
ptcld_validation_data %>% 
  dplyr::ungroup() %>% 
  dplyr::select(f_score) %>% 
  dplyr::summarise(
    dplyr::across(
      dplyr::everything()
      , .fns = list(
        mean = ~ mean(.x,na.rm=T), median = ~ median(.x,na.rm=T), sd = ~ sd(.x,na.rm=T)
        , min = ~ min(.x,na.rm=T), max = ~ max(.x,na.rm=T)
        , q25 = ~ quantile(.x, 0.25, na.rm = T)
        , q75 = ~ quantile(.x, 0.75, na.rm = T)
      )
      , .names = "{.fn}"
    )
  ) %>% 
  tidyr::pivot_longer(everything()) %>% 
  kableExtra::kbl(caption = "summary: `f_score`", digits = 3, col.names = NULL) %>% 
  kableExtra::kable_styling()
```

### Bayesian{#f_5_pred_mod_bays}

```{r, include=FALSE, eval=FALSE}
# get the levels of the nominal predictors
# see Kruschke (2015) "Nonadditive interaction of nominal predictors" on p.432
# If there are J levels of predictor 1 and K levels of predictor 2, then there are J × K combinations of the two predictors
### j is the depth map generation quality setting
ptcld_validation_data %>% dplyr::distinct(depth_maps_generation_quality) %>% dplyr::glimpse()
###  k is the depth map filtering mode setting
ptcld_validation_data %>% dplyr::distinct(depth_maps_generation_filtering_mode) %>% dplyr::glimpse()
### f is the processing software
ptcld_validation_data %>% dplyr::distinct(software) %>% dplyr::glimpse()
### s is the study site
ptcld_validation_data %>% dplyr::distinct(study_site) %>% dplyr::glimpse()

### j is the depth map generation quality setting
###  k is the depth map filtering mode setting
tidyr::crossing(
    depth_maps_generation_quality = unique(ptcld_validation_data$depth_maps_generation_quality)
    , depth_maps_generation_filtering_mode = unique(ptcld_validation_data$depth_maps_generation_filtering_mode)
  ) %>%
  dplyr::glimpse()

# priors?
formula_temp = brms::bf( f_score ~ 
    # baseline
    1 + 
    # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    (1 | study_site) + # only fitting main effects of site and not interactions
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
)
brms::get_prior(
  formula_temp,
  data = ptcld_validation_data,
  family = Beta(link = "logit")
)

```

With the beta likelihood our model with three nominal predictor variables and subject-level effects the model becomes:

\begin{align*}
y_{i} \sim & \operatorname{Beta} \bigl(\mu_{i}, \phi \bigr) \\
\operatorname{logit}(\mu_{i}) = & \beta_0 \\
& + \sum_{j=1}^{J=5} \beta_{1[j]} x_{1[j]} + \sum_{k=1}^{K=4} \beta_{2[k]} x_{2[k]} + \sum_{f=1}^{F=3} \beta_{3[f]} x_{3[f]} + \sum_{s=1}^{S=5} \beta_{4[s]} x_{4[s]}  \\
& + \sum_{j,k} \beta_{1\times2[j,k]} x_{1\times2[j,k]} + \sum_{j,f} \beta_{1\times3[j,f]} x_{1\times3[j,f]} + \sum_{k,f} \beta_{2\times3[k,f]} x_{2\times3[k,f]} \\
& + \sum_{j,k,f} \beta_{1\times2\times3[j,k,f]} x_{1\times2\times3[j,k,f]} \\
\beta_{0}  \sim & \operatorname{Normal}(0,1) \\ 
\beta_{1[j]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1}}) \\ 
\beta_{2[k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2}}) \\ 
\beta_{3[f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{3}}) \\ 
\beta_{4[s]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{4}}) \\ 
\beta_{1\times2[j,k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times2}}) \\ 
\beta_{1\times3[j,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times3}}) \\ 
\beta_{2\times3[k,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2\times3}}) \\ 
\sigma_{\beta_{1}} \sim & \operatorname{Student T}(3,0,2.5) \\ 
\sigma_{\beta_{2}} \sim & \operatorname{Student T}(3,0,2.5) \\ 
\sigma_{\beta_{3}} \sim & \operatorname{Student T}(3,0,2.5) \\ 
\sigma_{\beta_{4}} \sim & \operatorname{Student T}(3,0,2.5) \\ 
\sigma_{\beta_{1\times2}} \sim & \operatorname{Student T}(3,0,2.5) \\ 
\sigma_{\beta_{1\times3}} \sim & \operatorname{Student T}(3,0,2.5) \\ 
\sigma_{\beta_{2\times3}} \sim & \operatorname{Student T}(3,0,2.5) \\ 
\phi \sim & \operatorname{Gamma}(0.1,0.1) \\ 
\end{align*}

, where $j$ is the depth map generation quality setting corresponding to observation $i$, $k$ is the depth map filtering mode setting corresponding to observation $i$, $f$ is the processing software corresponding to observation $i$, and $s$ is the study site corresponding to observation $i$

Per [`brms`](https://cran.r-project.org/web/packages/brms/vignettes/brms_families.html), our $y$ is $\operatorname{Beta}$ distributed with the mean as $\mu$ and the concentration as $\phi$ which is sometimes called the *concentration*, *sample size* or *precision*. We can think of mean ($\mu$) and precision ($\phi$) just like with a normal distribution and its mean and standard deviation.

`brms` allows us to model the precision ($\phi$) but it is not required. If $\phi$ is not modeled, you still get a precision component, but it is universal across all the different coefficients (it doesn’t vary across any variables in the model). [Heiss](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#b-beta-regression-bayesian-style) explains that:

>for whatever mathy reasons, when you don’t explicitly model the precision, the resulting coefficient in the table isn’t on the log scale—it’s a regular non-logged number, so there’s no need to exponentiate.

in thie `brms` [community post](https://discourse.mc-stan.org/t/understanding-parameters-of-beta-family-in-brms/21640/8) it is similarly noted that:

>If you don’t predict the parameters, you give priors for them on non-transformed scale. When you predict them, the predictors become linear coefficients as any other and work on the transformed scale - the transformations are specified by the link_XX parameters of the families (and you can change them if you need).

#### Prior distributions

```{r}
#### setting priors
# required libraries: tidyverse, tidybayes, brms, palettetown, latex2exp
brms_f_mod5_priors_temp <- c(
    brms::prior(normal(0, 1), class = "Intercept")
    , brms::prior(student_t(3, 0, 2.5), class = "sd")
    , brms::prior(gamma(0.1, 0.1), class = phi)
  )
# plot
brms_f_mod5_priors_temp %>% 
  tidybayes::parse_dist() %>% 
  tidybayes::marginalize_lkjcorr(K = 2) %>% 
  mutate(
    distrib = case_when(
      prior == "student_t(3, 0, 2.5)" ~ latex2exp::TeX(r'($\sigma \sim Student\,T(3, 0, 2.5)$)', output = "character"),
      prior == "normal(0, 1)" ~ latex2exp::TeX(r'($\beta \sim Normal(0, 1)$)',  output = "character"),
      prior == "gamma(0.1, 0.1)" ~ latex2exp::TeX(r'($\phi \sim$ Gamma(0.1, 0.1))',  output = "character")
  )) %>% 
  ggplot(., aes(dist = .dist, args = .args)) +
  facet_wrap(vars(distrib), scales = "free", labeller = label_parsed) +
  ggdist::stat_halfeye(
    aes(fill = prior),
    n = 10e2,
    show.legend = F
    , fill = "slategray"
  ) +
  coord_flip() + 
  theme_light() +
  theme(
    strip.text = element_text(face = "bold", color = "black"),
    axis.text.y = element_blank(),
    axis.ticks = element_blank()
    , plot.subtitle = element_text(size = 8)
    , plot.title = element_text(size = 9)
  )+
  labs(
    x = "",
    title = "Priors: F-Score",
    y = "Log Odds"
  ) 
```

#### Fit the model

Now fit the model.

```{r}
brms_f_mod5 = brms::brm(
  formula = f_score ~ 
    # baseline
    1 + 
    # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    (1 | study_site) + # only fitting main effects of site and not interactions
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
  , data = ptcld_validation_data
  , family = Beta(link = "logit")
  # priors
  , prior = brms_f_mod5_priors_temp
  # mcmc
  , iter = 20000, warmup = 10000, chains = 4
  , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = round(parallel::detectCores()/2)
  , file = paste0(rootdir, "/fits/brms_f_mod5")
)
# brms::make_stancode(brms_f_mod5)
# brms::prior_summary(brms_f_mod5)
# print(brms_f_mod5)
# brms::neff_ratio(brms_f_mod5)
# brms::rhat(brms_f_mod5)
# brms::nuts_params(brms_f_mod5)
```

check the prior distributions

```{r}
# check priors
brms::prior_summary(brms_f_mod5) %>%
  kableExtra::kbl() %>% 
  kableExtra::kable_styling()
```

The `brms::brm` model summary

We won't clutter the output here but this can be run if you are following along on your own

```{r, eval=F}
brms_f_mod5 %>% 
  brms::posterior_summary() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | stringr::str_starts(parameter, "sd_") 
    | parameter == "phi"
  ) %>%
  dplyr::mutate(
    parameter = parameter %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
  ) %>% 
  kableExtra::kbl(digits = 2, caption = "Bayesian final model for F-score") %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "8in")
```

#### Posterior Predictive Checks{#ppcheck_mod5}

Markov chain Monte Carlo (MCMC) simulations were conducted using the `brms` package (Bürkner 2017) to estimate posterior predictive distributions of the parameters of interest. We ran three chains of 100,000 iterations with the first 50,000 discarded as burn-in. Trace-plots were utilized to visually assess model convergence.

check the trace plots for problems with convergence of the Markov chains

```{r, fig.height=8}
plot(brms_f_mod5)
```

Sufficient convergence was checked with $\hat{R}$ values near 1 ([Brooks & Gelman, 1998](https://scholar.google.com/scholar?cluster=14209404114665352991&hl=en&as_sdt=0,6)). 

check our $\hat{R}$ values

```{r}
brms::mcmc_plot(brms_f_mod5, type = "rhat_hist") +
  theme_light() +
  theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

and another check of our $\hat{R}$ values

```{r}
brms_f_mod5 %>% 
  brms::rhat() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::rename(rhat = 2) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | stringr::str_starts(parameter, "sd_") 
    | parameter == "phi"
  ) %>%
  dplyr::mutate(
    parameter = parameter %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
    , chk = (rhat <= 1*0.998 | rhat >= 1*1.002)
  ) %>% 
  ggplot(aes(x = rhat, y = parameter, color = chk, fill = chk)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "gray44", lwd = 1.2) +
  geom_vline(xintercept = 1*0.998, lwd = 1.5) +
  geom_vline(xintercept = 1*1.002, lwd = 1.5) +
  geom_vline(xintercept = 1*0.999, lwd = 1.2, color = "gray33") +
  geom_vline(xintercept = 1*1.001, lwd = 1.2, color = "gray33") +
  geom_point() +
  scale_fill_manual(values = c("navy", "firebrick")) +
  scale_color_manual(values = c("navy", "firebrick")) +
  scale_y_discrete(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\hat{R}$")
    , subtitle = latex2exp::TeX("MCMC chain convergence check for $\\hat{R}$ values")
    , title = "F-Score"
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , axis.text.y = element_text(size = 4)
    , panel.grid.major.x = element_blank()
    , panel.grid.minor.x = element_blank()
    , plot.subtitle = element_text(size = 8)
    , plot.title = element_text(size = 9)
  )
```

The effective length of an MCMC chain is indicated by the effective sample size (ESS), which refers to the sample size of the MCMC chain **not** to the sample size of the data
[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) notes: 

>One simple guideline is this: For reasonably accurate and stable estimates of the limits of the 95% HDI, an ESS of 10,000 is recommended. This is merely a heuristic based on experience with practical applications, it is not a requirement. If accuracy of the HDI limits is not crucial for your application, then a smaller ESS may be sufficient. (p.184)

```{r}
# get ess values from model summary
dplyr::bind_rows(
    summary(brms_f_mod5) %>% 
      purrr::pluck("random") %>% 
      purrr::flatten() %>% 
      purrr::keep_at(~ .x == "Bulk_ESS") %>% 
      unlist() %>% 
      dplyr::as_tibble()
    , summary(brms_f_mod5) %>% 
      purrr::pluck("fixed") %>% 
      purrr::flatten() %>% 
      purrr::keep_at(~ .x == "Bulk_ESS") %>% 
      unlist() %>% 
      dplyr::as_tibble()
  ) %>% 
  dplyr::rename(ess = 1) %>% 
  dplyr::mutate(parameter = dplyr::row_number(), chk = ess<10000) %>% 
  ggplot(aes(x = ess, y = parameter, color = chk, fill = chk)) +
  geom_vline(xintercept = 10000, linetype = "dashed", color = "gray44", lwd = 1.2) +
  geom_segment( aes(x = 0, xend=ess, yend=parameter), color="black") +
  geom_point() +
  scale_fill_manual(values = c("blue4", "blue3")) +
  scale_color_manual(values = c("blue4", "blue3")) +
  scale_y_continuous(NULL, breaks = NULL) +
  scale_x_continuous(labels = scales::comma) +
  labs(
    x = "ESS"
    , subtitle = "MCMC chain resolution check for effective sample size (ESS) values"
    , y = ""
    , title = "F-Score"
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , axis.text.y = element_text(size = 4)
    , panel.grid.major.x = element_blank()
    , panel.grid.minor.x = element_blank()
    , plot.subtitle = element_text(size = 8)
    , plot.title = element_text(size = 9)
  )
```

and another effective sample size check

```{r}
brms::mcmc_plot(brms_f_mod5, type = "neff_hist") +
  theme_light() +
  theme(
    legend.position = "top", legend.direction = "horizontal"
  )
```

Posterior predictive checks were used to evaluate model goodness-of-fit by comparing data simulated from the model with the observed data used to estimate the model parameters ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)). Calculating the proportion of MCMC iterations in which the test statistic (i.e., mean and sum of squares) from the simulated data and observed data are more extreme than one another provides the Bayesian P-value. Lack of fit is indicated by a value close to 0 or 1 while a value of 0.5 indicates perfect fit ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)).

To learn more about this approach to posterior predictive checks, check out Gabry’s (2022) vignette, [Graphical posterior predictive checks using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html). 

posterior-predictive check to make sure the model does an okay job simulating data that resemble the sample data

```{r}
# posterior predictive check
brms::pp_check(
    brms_f_mod5
    , type = "dens_overlay"
    , ndraws = 100
  ) + 
  labs(subtitle = "posterior-predictive check (overlaid densities)") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
    , plot.subtitle = element_text(size = 8)
    , plot.title = element_text(size = 9)
  )

ggplot2::ggsave("../data/ppchk_ovrll_mod5.png", height = 7, width = 10.5)
```

another way

```{r}
brms::pp_check(brms_f_mod5, type = "ecdf_overlay", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (ECDF: empirical cumulative distribution function)") + 
  theme_light() +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

and another posterior predictive check for the overall model

```{r}
# means
p1_temp = brms::pp_check(
    brms_f_mod5
    , type = "stat"
    , stat = "mean"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
# sds
p2_temp = brms::pp_check(
    brms_f_mod5
    , type = "stat"
    , stat = "sd"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "sd's") +
  theme_light()

# combine 
(p1_temp + p2_temp) &
  theme(legend.position = "none") &
  plot_annotation(
    title = "Posterior-predictive statistical checks\noverall model"
    , subtitle = expression(
      "The dark blue lines are "*italic(T(y))*", and the light blue bars are for "*italic(T)(italic(y)[rep])*".")
  )
```

and another posterior predictive check for the overall model combining mean and sd

```{r}
brms::pp_check(brms_f_mod5, type = "stat_2d") +
  theme_light() +
  labs(title = "F-Score") +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 8)
    , plot.title = element_text(size = 9)
  )
```

How’d we do capturing the conditional means and standard deviations by depth map generation quality?

```{r}
# means
p1_temp = brms::pp_check(
    brms_f_mod5
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "mean"
    , group = "depth_maps_generation_quality"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  facet_grid(cols = vars(group), scales = "free") +
  theme_light()
# sds
p2_temp = brms::pp_check(
    brms_f_mod5
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "sd"
    , group = "depth_maps_generation_quality"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "sd's") +
  facet_grid(cols = vars(group), scales = "free") +
  theme_light()
# combine 
(p1_temp / p2_temp) &
  theme(legend.position = "none") &
  plot_annotation(
    title = "Posterior-predictive statistical checks\nby dense cloud quality"
    , subtitle = expression(
      "The dark blue lines are "*italic(T(y))*", and the light blue bars are for "*italic(T)(italic(y)[rep])*".")
  )

ggplot2::ggsave("../data/ppchk_qlty_mod5.png", height = 7, width = 10.5)
```

Both the means and sd's of the F-score are well represented across the different levels of dense cloud quality

What about for the software?

```{r}
pp_check(brms_f_mod5, "dens_overlay_grouped", group = "software", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (overlaid densities)\nby software") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )

ggplot2::ggsave("../data/ppchk_sftwr_mod5.png", height = 7, width = 10.5)
```

and what about for the filtering mode?

```{r}
pp_check(brms_f_mod5, "dens_overlay_grouped", group = "depth_maps_generation_filtering_mode", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (overlaid densities)\nby filtering mode") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

It looks like our model is making predictions that are consistent with our original data, which is what we want.

We can look at the model noise standard deviation (concentration) $\phi$ and the intercept

We can think of mean ($\mu$) and precision ($\phi$) just like with a normal distribution and its mean and standard deviation.

```{r}
# get formula
form_temp = brms_f_mod5$formula$formula[3] %>% 
  as.character() %>% get_frmla_text() %>% 
  stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
  stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
# extract the posterior draws
brms::as_draws_df(brms_f_mod5) %>% 
# plot
  ggplot(aes(x = phi, y = 0)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21, point_size = 3
    , quantiles = 100
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\phi$")
    # , caption = form_temp
  ) +
  theme_light()

ggplot2::ggsave("../data/phi_posterior_mod5.png", height = 7, width = 10.5)
```

#### Quality:Filtering - interaction

Are there differences in F-score based on dense point cloud generation quality within each level of filtering mode?

Here, we collapse across the study site and software to compare the combined dense cloud quality and filtering mode effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
qlty_filter_draws_temp = 
  ptcld_validation_data %>% 
    dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod5, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_quality) +
        (1 | depth_maps_generation_filtering_mode) + 
        (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    dplyr::mutate(med = tidybayes::median_hdci(value)$y)
# plot
  qlty_filter_draws_temp %>% 
    dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = depth_maps_generation_filtering_mode
      # , fill = depth_maps_generation_filtering_mode
      , fill = med
    )
  ) +
    tidybayes::stat_eye(
      point_interval = median_hdi, .width = .95
      , slab_alpha = 0.95
      , interval_color = "black", linewidth = 1
      , point_color = "black", point_fill = "black", point_size = 1
    ) +
    # scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_fill_viridis_c(option = "mako", direction=-1, begin = 0.2, end = 0.8, limits = c(0,1)) +
    scale_y_continuous(limits = c(0,1), breaks = scales::extended_breaks(n=8)) +
    facet_grid(cols = vars(depth_maps_generation_quality)) +
    labs(
      x = "filtering mode", y = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
      , fill = "Filtering Mode"
      # , caption = form_temp
    ) +
    theme_light() +
    theme(
      legend.position = "none"
      , legend.direction  = "horizontal"
      , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
      , strip.text = element_text(color = "black", face = "bold")
    ) 
ggplot2::ggsave(
  "../data/qlty_fltr_mod5.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and a table of these 95% HDI values

```{r}
table_temp =
  qlty_filter_draws_temp %>% 
    tidybayes::median_hdi(value) %>% 
    dplyr::select(-c(.point,.interval, .width,.row)) %>% 
    dplyr::arrange(depth_maps_generation_quality, depth_maps_generation_filtering_mode)

table_temp %>%
  dplyr::select(-c(depth_maps_generation_quality)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "F-score<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "filtering mode"
      , "F-score<br>median"
      , "HDI low", "HDI high"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$depth_maps_generation_quality))) %>% 
  kableExtra::scroll_box(height = "8in")
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
brms_contrast_temp = qlty_filter_draws_temp %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = 
        contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
    , depth_maps_generation_filtering_mode = depth_maps_generation_filtering_mode %>% 
      factor(
        levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
        , ordered = T
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast,depth_maps_generation_filtering_mode) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "depth_maps_generation_filtering_mode"
  , label_size = 2.0
  , x_expand = c(0,0.6)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby filtering mode"
  ) +
  theme(
    axis.text.x = element_text(size = 7)
  )
ggplot2::ggsave(
  "../data/qlty_fltr_comp_mod5.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
table_temp = 
  brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_filtering_mode, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_filtering_mode) %>% 
  dplyr::select(contrast, depth_maps_generation_filtering_mode, value, .lower, .upper, pr_gt_zero) %>% 
  dplyr::arrange(contrast, depth_maps_generation_filtering_mode)

table_temp %>% 
  dplyr::select(-c(contrast)) %>% 
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "filtering mode"
      , "median difference<br>F-score"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$contrast))) %>% 
  kableExtra::scroll_box(height = "8in")
```

#### Software:Quality - interaction

It might be more important to understand the difference in F-score by dense cloud quality and software rather than filtering mode since filtering mode had such a small effect on the SfM predictive ability

Are there differences in F-score based on dense point cloud generation quality within each different processing software? We will also address the similar but slightly different question of "are there differences in F-score based on the processing software used at a given dense point cloud generation quality?"

Here, we collapse across the study site and filtering mode to compare the combined dense cloud quality and software effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
# get draws
qlty_sftwr_draws_temp = 
  tidyr::crossing(
    depth_maps_generation_quality = unique(ptcld_validation_data$depth_maps_generation_quality)
    , software = unique(ptcld_validation_data$software)
  ) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | software) + 
      (1 | depth_maps_generation_quality:software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(
    med = tidybayes::median_hdci(value)$y
  ) 

# plot
qlty_sftwr_draws_temp %>% 
  # remove out-of-sample obs
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(depth_maps_generation_quality, software)
    , by = dplyr::join_by(depth_maps_generation_quality, software)
  ) %>% 
  dplyr::mutate(
    depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()
  ) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = software
      # , fill = software
      , fill = med
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  # scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
  scale_fill_viridis_c(option = "mako", direction=-1, begin = 0.2, end = 0.8, limits = c(0,1)) +
  scale_y_continuous(limits = c(0,1), breaks = scales::extended_breaks(n=8)) +
  facet_grid(cols = vars(depth_maps_generation_quality)) +
  labs(
    x = "software", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
    # , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
  ) 
```

and a table of these 95% HDI values

```{r}
table_temp = 
  qlty_sftwr_draws_temp %>% 
    tidybayes::median_hdi(value) %>% 
    # remove out-of-sample obs
    dplyr::inner_join(
      ptcld_validation_data %>% dplyr::distinct(depth_maps_generation_quality, software)
      , by = dplyr::join_by(depth_maps_generation_quality, software)
    ) %>% 
    dplyr::select(-c(.point,.interval, .width,.row)) %>% 
    dplyr::arrange(software,depth_maps_generation_quality)
  
table_temp %>% 
  dplyr::select(-c(software)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "F-score<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "quality"
      , "F-score<br>median"
      , "HDI low", "HDI high"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$software))) %>% 
  kableExtra::scroll_box(height = "8in")
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
# calculate contrast
brms_contrast_temp = qlty_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_quality
    , comparison = contrast_list
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software) %>% 
  make_contrast_vars()

# remove out-of-sample obs
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(software, sorter1 == depth_maps_generation_quality)
  ) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(software, sorter2 == depth_maps_generation_quality)
  )
  
# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "software"
  , label_size = 2.0
  , x_expand = c(0.56,0.9) # c(0,0.7)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby software"
  )

ggplot2::ggsave(
  "../data/qlty_sftwr_comp_mod5.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

create plot for combining with other contrasts for publication

```{r}
ptchwrk_qlty_sftwr_comp =
  plt_contrast(
    brms_contrast_temp
    , y_axis_title = "quality contrast"
    , facet = "software"
    , label_size = 1.35
    , label = "pr_diff_lab_sm"
    , annotate_size = 1.75
  ) +
    labs(
      subtitle = ""
      , x = "F-score contrast"
    ) +
    theme(
      legend.position="none"
      , axis.title.y = element_text(size = 10, face = "bold")
      , axis.title.x = element_text(size = 8)
    ) 
# ptchwrk_qlty_sftwr_comp
```

```{r, include=FALSE, eval=FALSE}
### old
# ptchwrk_qlty_sftwr_comp_dta = brms_contrast_temp
ptchwrk_qlty_sftwr_comp =
  plt_contrast(
    brms_contrast_temp
    , y_axis_title = "quality contrast"
    , facet = "software"
    , label_size = 1.4
    , label = "pr_diff_lab_sm"
    , annotate_size = 1.8
  ) +
    labs(
      subtitle = ""
      , x = "F-score contrast"
    ) +
    theme(
      legend.position="none"
      , axis.title.y = element_text(size = 10, face = "bold")
      , axis.title.x = element_text(size = 8)
    ) 
# ptchwrk_qlty_sftwr_comp

```

and summarize these contrasts

```{r}
table_temp = brms_contrast_temp %>%
  dplyr::group_by(contrast, software, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, software) %>% 
  dplyr::select(contrast, software, value, .lower, .upper, pr_gt_zero) %>% 
  dplyr::arrange(software, contrast)

table_temp %>% 
  dplyr::select(-c(software)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "median difference<br>F-score"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$software))) %>% 
  kableExtra::scroll_box(height = "8in")
```

The contrasts above address the question "are there differences in F-score based on dense point cloud generation quality within each software?".

To address the different question of "are there differences in F-score based on the processing software used at a given dense point cloud generation quality?" we need to utilize a different formulation of the `comparison` parameter within our call to the `tidybayes::compare_levels` function and calculate the contrast by `software` instead

```{r}
# calculate contrast
brms_contrast_temp = qlty_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = software
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = software) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, depth_maps_generation_quality) %>% 
  make_contrast_vars()

# remove out-of-sample obs
brms_contrast_temp = brms_contrast_temp %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(sorter1 == software, depth_maps_generation_quality)
  ) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(sorter2 == software, depth_maps_generation_quality)
  )
# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "software"
  , facet = "depth_maps_generation_quality"
  , label_size = 2.0
  , x_expand = c(0.17,0.14)
) +
  facet_wrap(facets = vars(depth_maps_generation_quality), ncol = 2) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby dense cloud quality"
  ) +
  theme(
    legend.position = c(.75, .13)
  ) +
  guides(fill = guide_colorbar(theme = theme(
    legend.key.width  = unit(1, "lines"),
    legend.key.height = unit(7, "lines")
  )))

ggplot2::ggsave(
  "../data/sftwr_qlty_comp_mod5.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

create plot for combining with other contrasts for publication

```{r}
ptchwrk_sftwr_qlty_comp =
  plt_contrast(
    brms_contrast_temp
    , y_axis_title = "software contrast"
    , facet = "depth_maps_generation_quality"
    , label_size = 1.7
    , label = "pr_diff_lab_sm"
    , annotate_size = 1.8
  ) +
    facet_wrap(facets = vars(depth_maps_generation_quality), ncol = 3) +
    labs(
      subtitle = ""
      , x = "F-score constrast"
    ) +
    theme(
      legend.position = "inside"
      , legend.position.inside = c(.8, .11)
      , axis.title.y = element_text(size = 10, face = "bold")
      , axis.title.x = element_text(size = 8)
    ) +
    guides(fill = guide_colorbar(theme = theme(
      legend.key.width  = unit(1, "lines"),
      legend.key.height = unit(7, "lines")
    )))
# ptchwrk_sftwr_qlty_comp
```

```{r, include=FALSE, eval=FALSE}
### old
# ptchwrk_sftwr_qlty_comp_dta = brms_contrast_temp
ptchwrk_sftwr_qlty_comp =
  plt_contrast(
    brms_contrast_temp
    , y_axis_title = "software contrast"
    , facet = "depth_maps_generation_quality"
    , label_size = 1.7
    , label = "pr_diff_lab_sm"
    , annotate_size = 1.8
  ) +
    facet_wrap(facets = vars(depth_maps_generation_quality), ncol = 2) +
    labs(
      subtitle = ""
      , x = "F-score constrast"
    ) +
    theme(
      legend.position = c(.75, .13)
      , axis.title.y = element_text(size = 10, face = "bold")
      , axis.title.x = element_text(size = 8)
    ) +
    guides(fill = guide_colorbar(theme = theme(
      legend.key.width  = unit(1, "lines"),
      legend.key.height = unit(7, "lines")
    )))
# ptchwrk_sftwr_qlty_comp

```

and summarize these contrasts

```{r}
table_temp = brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_quality, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_quality) %>% 
  dplyr::select(contrast, depth_maps_generation_quality, value, .lower, .upper, pr_gt_zero)
   
table_temp %>% 
  dplyr::select(-c(contrast)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality"
      , "median difference<br>F-score"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$contrast))) %>% 
  kableExtra::scroll_box(height = "8in")
```

#### Software:Filtering - interaction

Are there differences in F-score based on dense point cloud filtering mode within each processing software?

Here, we collapse across the study site and depth map generation quality to compare the combined filtering mode and software effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

Even though filtering mode had a small effect on the SfM predictive ability when averaging across all softwares, there might still be differences in filtering mode within software when we average across all depth map generation quality settings. Let's check the difference in F-score by depth map filtering mode and software.

```{r}
# get draws
fltr_sftwr_draws_temp = ptcld_validation_data %>%
  dplyr::distinct(depth_maps_generation_filtering_mode, software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode) +
      (1 | software) + 
      (1 | depth_maps_generation_filtering_mode:software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(med = tidybayes::median_hdci(value)$y)

  # plot
fltr_sftwr_draws_temp %>% 
  ggplot(
    mapping = aes(
      y = value, x = software
      # , fill = software
      , fill = med
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  # scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
  scale_fill_viridis_c(option = "mako", direction=-1, begin = 0.2, end = 0.8, limits = c(0,1)) +
  scale_y_continuous(limits = c(0,1), breaks = scales::extended_breaks(n=8)) +
  facet_grid(cols = vars(depth_maps_generation_filtering_mode)) +
  labs(
    x = "software", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby filtering mode"
    # , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
  ) 
```

and a table of these 95% HDI values

```{r}
table_temp = 
  fltr_sftwr_draws_temp %>% 
    tidybayes::median_hdi(value) %>% 
    dplyr::select(-c(.point,.interval, .width,.row)) %>% 
    dplyr::arrange(software,depth_maps_generation_filtering_mode)

table_temp %>% 
  dplyr::select(-c(software)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "F-score<br>95% HDI of the posterior predictive distribution"
    , col.names = c(
      "filtering mode"
      , "F-score<br>median"
      , "HDI low", "HDI high"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$software))) %>% 
  kableExtra::scroll_box(height = "8in")
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
# calculate contrast
brms_contrast_temp = fltr_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_filtering_mode
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_filtering_mode)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      ) %>% 
      # re order for filtering mode
      forcats::fct_rev()
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "filtering mode"
  , facet = "software"
  , label_size = 2.0
  , x_expand = c(1.8,1.8) # c(1,1.4)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby software"
  )

ggplot2::ggsave(
  "../data/fltr_sftwr_comp_mod5.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

create plot for combining with other RMSE contrasts for publication

```{r}
ptchwrk_fltr_sftwr_comp =
  plt_contrast(
    brms_contrast_temp
    , y_axis_title = "filtering mode contrast"
    , facet = "software"
    , label_size = 1.7
    , label = "pr_diff_lab_sm"
    , annotate_size = 1.8
  ) +
    labs(
      subtitle = "" # "constrast Height RMSE (m)"
      , x = "F-score constrast"
    ) +
    theme(
      legend.position="none"
      , axis.title.y = element_text(size = 10, face = "bold")
      , axis.title.x = element_text(size = 8)
    ) 
# ptchwrk_fltr_sftwr_comp
```

```{r, include=FALSE, eval=FALSE}
### old
# ptchwrk_fltr_sftwr_comp_dta = brms_contrast_temp
ptchwrk_fltr_sftwr_comp =
  plt_contrast(
    brms_contrast_temp
    , y_axis_title = "filtering mode contrast"
    , facet = "software"
    , label_size = 1.7
    , label = "pr_diff_lab_sm"
    , annotate_size = 1.8
  ) +
    labs(
      subtitle = "" # "constrast Height RMSE (m)"
      , x = "F-score constrast"
    ) +
    theme(
      legend.position="none"
      , axis.title.y = element_text(size = 10, face = "bold")
      , axis.title.x = element_text(size = 8)
    ) 
# ptchwrk_fltr_sftwr_comp

```

and summarize these contrasts

```{r}
table_temp = brms_contrast_temp %>%
  dplyr::group_by(contrast, software, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, software) %>% 
  dplyr::select(contrast, software, value, .lower, .upper, pr_gt_zero) %>% 
  dplyr::arrange(software, contrast)

table_temp %>% 
  dplyr::select(-c(software)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "filtering contrast"
      , "median difference<br>F-score"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$software))) %>% 
  kableExtra::scroll_box(height = "8in")
```

#### Software:Quality:Filtering - interaction

The contrasts immediately above address the question "are there differences in F-score based on dense point cloud filtering mode within each software?". Although the impact of filtering mode is small, it is highly probable when averaging across all quality settings. What if we don't average out the impact of quality and instead get the full, three-way interaction between software, quality, and filtering mode?

Let's get the model's answer to the question "For each software, are there differences in F-score based on dense point cloud filtering mode within each point cloud generation quality?".

Here, we collapse across the study site to compare the dense cloud quality, filtering mode, and software effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
# get draws
fltr_sftwr_draws_temp =
  tidyr::crossing(
    depth_maps_generation_quality = unique(ptcld_validation_data$depth_maps_generation_quality)
    , depth_maps_generation_filtering_mode = unique(ptcld_validation_data$depth_maps_generation_filtering_mode)
    , software = unique(ptcld_validation_data$software)
  ) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) + # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(med = tidybayes::median_hdci(value)$y)
# let's add the grand mean to the plot for reference
grand_mean_temp = brms::posterior_summary(brms_f_mod5) %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::filter(
    parameter == "Intercept"
  ) %>% 
  dplyr::mutate(dplyr::across(-c(parameter), ~plogis(.))) %>% 
  dplyr::pull(estimate)
# plot
fltr_sftwr_draws_temp %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
    , by = dplyr::join_by(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  ggplot(
    mapping = aes(
      y = value
      , x = depth_maps_generation_filtering_mode
      # , fill = depth_maps_generation_filtering_mode
      , fill = med
    )
  ) +
  geom_hline(yintercept = grand_mean_temp, color = "gray33") +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.95
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  # scale_fill_viridis_d(option = "plasma", drop = F) +
  scale_fill_viridis_c(option = "mako", direction=-1, begin = 0.2, end = 0.8, limits = c(0,1)) +
  scale_y_continuous(limits = c(0,1), breaks = scales::extended_breaks(n=8)) +
  facet_grid(
    rows = vars(software)
    , cols = vars(depth_maps_generation_quality)
    # , switch = "y"
  ) +
  labs(
    fill = ""
    , x = "filtering mode", y = "F-score"
    # , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality and software"
    , subtitle = "quality"
    # , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
    , plot.subtitle = element_text(hjust = 0.5)
    , panel.grid = element_blank()
    # , strip.placement = "outside"
  ) +
  guides(
    fill = guide_legend(override.aes = list(shape = NA, size = 6, alpha = 0.9, lwd = NA))
  )

ggplot2::ggsave(
  "../data/qlty_fltr_sftwr_mod5.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

let's add a table of the results

```{r}
table_temp =
  fltr_sftwr_draws_temp %>% 
  tidybayes::median_hdi(value) %>%
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
    , by = dplyr::join_by(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>%
  dplyr::select(c(
    software, depth_maps_generation_quality, depth_maps_generation_filtering_mode
    , value, .lower, .upper
  )) %>% 
  dplyr::ungroup() %>% 
  dplyr::arrange(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)

table_temp %>% 
  # dplyr::select(-c(software)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution"
    , col.names = c(
      "software", "quality", "filtering mode"
      , "F-score<br>median"
      , "HDI low", "HDI high"
    )
    , escape = F
  ) %>% 
  kableExtra::kable_styling() %>% 
  # kableExtra::pack_rows(index = table(forcats::fct_inorder(table_temp$software))) %>% 
  kableExtra::collapse_rows(columns = 1:2, valign = "top") %>%
  kableExtra::scroll_box(height = "8in")
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
# calculate contrast
brms_contrast_temp = fltr_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_filtering_mode
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_filtering_mode)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      ) %>% 
      # re order for filtering mode
      forcats::fct_rev()
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software, depth_maps_generation_quality) %>% 
  make_contrast_vars()

# remove out-of-sample obs
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
    , by = dplyr::join_by(software, depth_maps_generation_quality, sorter1==depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
    , by = dplyr::join_by(software, depth_maps_generation_quality, sorter2==depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev())

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
brms_contrast_temp %>% 
  plt_contrast(
    facet = c("depth_maps_generation_quality", "software")
    , y_axis_title = "filtering mode"
    , label_size = 0
    , x_expand = c(-0.1,-0.1)
  ) +
  facet_grid(
    rows = vars(software)
    , cols = vars(depth_maps_generation_quality)
  ) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby dense cloud quality and software"
  )
  
ggplot2::ggsave(
  "../data/qlty_fltr_sftwr_comp_mod5.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

#### Quality - main effect

let's collapse across the filtering mode, software, and study site to compare the dense cloud quality setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(med = tidybayes::median_hdci(value)$y) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_quality
      # , fill = depth_maps_generation_quality
      , fill = med
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    # scale_fill_viridis_d(option = "inferno", drop = F) +
    scale_fill_viridis_c(option = "mako", direction=-1, begin = 0.2, end = 0.8, limits = c(0,1)) +
    scale_x_continuous(limits = c(0,1), breaks = scales::extended_breaks(n=8)) +
    # scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "quality", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      # , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [one nominal predictor model above](#f_one_pred_mod_bays), [two nominal predictor model above](#f_two_pred_mod_bays), [three nominal predictor model above](#f_three_pred_mod_bays), and [three nominal predictor + site effects model above](#f_4_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod5") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod4
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_quality)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod4")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod3
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_quality)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod3")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod2
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_quality)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod2")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_quality) %>% 
      tidybayes::add_epred_draws(brms_f_mod1) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod1")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(depth_maps_generation_quality), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  # scale_color_viridis_d(option = "turbo", begin = 0.2, end = 0.8) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)) +
  labs(
    y = "", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

we can also perform pairwise comparisons after collapsing across the filtering mode, software, and study site to compare the dense cloud quality setting effect

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(depth_maps_generation_quality) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod5, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_quality)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = 
        contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "quality"
)

```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast) %>% 
  dplyr::select(contrast, value, .lower, .upper, pr_gt_zero) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()
```

#### Filtering - main effect

let's collapse across the dense cloud quality, software, and study site to compare the dense cloud quality setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(med = tidybayes::median_hdci(value)$y) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_filtering_mode
      # , fill = depth_maps_generation_filtering_mode
      , fill = med
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    # scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_fill_viridis_c(option = "mako", direction=-1, begin = 0.2, end = 0.8, limits = c(0,1)) +
    scale_x_continuous(limits = c(0,1), breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "filtering mode", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      # , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [two nominal predictor model above](#f_two_pred_mod_bays), [three nominal predictor model above](#f_three_pred_mod_bays), and [three nominal predictor + site model above](#f_4_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod5") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod4
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod4")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod3
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod3")
    , ptcld_validation_data %>% 
      dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod2
        # this part is crucial
        , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod2")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(depth_maps_generation_filtering_mode), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[2:5]) +
  labs(
    y = "filtering mode", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

we can also perform pairwise comparisons after collapsing across the dense cloud quality, software, and study site to compare the filtering mode setting effect.

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod5, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_filtering_mode
      , comparison = "pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_filtering_mode)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      ) %>% 
      # re order for filtering mode
      forcats::fct_rev()
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "filtering mode"
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast) %>% 
  dplyr::select(contrast, value, .lower, .upper, pr_gt_zero) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "filtering mode contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()
```

#### Software - main effect

to address one of our main questions, let's also collapse across the study site, dense cloud quality, and filtering mode setting to compare the software effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5
    # this part is crucial
    , re_formula = ~ (1 | software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(med = tidybayes::median_hdci(value)$y) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = software
      # , fill = software
      , fill = med
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    # scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
    scale_fill_viridis_c(option = "mako", direction=-1, begin = 0.2, end = 0.8, limits = c(0,1)) +
    scale_x_continuous(limits = c(0,1), breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "software", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      # , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

let's compare these results to the results from our [three nominal predictor model above](#f_three_pred_mod_bays) and [three nominal predictor + site effects model above](#f_4_pred_mod_bays)

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod5
    # this part is crucial
    , re_formula = ~ (1 | software)
  ) %>% 
  dplyr::mutate(value = .epred, src = "brms_f_mod5") %>%
  dplyr::bind_rows(
    ptcld_validation_data %>% 
      dplyr::distinct(software) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod4
        # this part is crucial
        , re_formula = ~ (1 | software)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod4")
    , ptcld_validation_data %>% 
      dplyr::distinct(software) %>% 
      tidybayes::add_epred_draws(
        brms_f_mod3
        # this part is crucial
        , re_formula = ~ (1 | software)
      ) %>% 
      dplyr::mutate(value = .epred, src = "brms_f_mod3")
  ) %>% 
  ggplot(mapping = aes(y = src, x = value, color = src, group = src)) +
  tidybayes::stat_pointinterval(position = "dodge") +
  facet_grid(rows = vars(software), switch = "y") +
  scale_y_discrete(NULL, breaks = NULL) +
  scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[3:5]) +
  labs(
    y = "", x = "F-score"
    , color = "model"
  ) +
  theme_light() +
  theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
  
```

we can also perform pairwise comparisons after collapsing across the filtering mode, dense cloud quality setting, and study site to compare the software main effect

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(software) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod5, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | software)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = software
      , comparison = "pairwise"
    ) %>% 
    dplyr::rename(contrast = software) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast) %>%
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp
  # , caption_text = form_temp
  , y_axis_title = "software"
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast) %>% 
  dplyr::select(contrast, value, .lower, .upper, pr_gt_zero) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "software contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()
```

#### $\sigma$ posteriors

Finally, we can quantify the variation in F-score by comparing the $\sigma$ (`sd`) posteriors

*unsure about the scale of the $\sigma$ parameters are on in the beta model. Here, we invert the logit `sd` values from the model using `plogis()` which converts the parameter values to a probability/proportion (e.g.; 0-1) because they are parameters of the intercept and interaction effects so must be on the transformed (`link = "logit"`) scale...double check*

For a phenomenally excellent overview of binary logistic regression and how to interpret coefficients, see Steven Miller’s most excellent lab [script here](https://post8000.svmiller.com/lab-scripts/logistic-regression-lab.html)

```{r}
# tidybayes::get_variables(brms_f_mod5)
# extract the posterior draws
# extract the posterior draws
  brms::as_draws_df(brms_f_mod5) %>%
    dplyr::select(c(tidyselect::starts_with("sd_"))) %>% 
    tidyr::pivot_longer(dplyr::everything()) %>% 
    # dplyr::group_by(name) %>% 
    # tidybayes::median_hdi(value) %>% 
    dplyr::mutate(
      name = name %>% 
        stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
        stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering") %>% 
        stringr::str_remove_all("sd_") %>% 
        stringr::str_remove_all("__Intercept") %>% 
        stringr::str_replace_all("_", " ") %>% 
        forcats::fct_reorder(value)
    ) %>%
  # plot
    ggplot(aes(x = value, y = name)) +
    tidybayes::stat_dotsinterval(
      point_interval = median_hdi, .width = .95
      , justification = -0.04
      , shape = 21 #, point_size = 3
      , quantiles = 100
    ) +
    scale_x_continuous(breaks = NULL) +
    labs(x = "", y = ""
      , subtitle = latex2exp::TeX("$\\sigma_\\beta$ posterior distributions")
      , title = "F-Score"
    ) +
    theme_light() +
    theme(
      plot.subtitle = element_text(size = 8)
      , plot.title = element_text(size = 9)
    )
```

Variance of study site is stronger than variance of depth map generation quality, but the posterior predictive distributions overlap a good deal. The study site (the "subjects" in our study) seems to have the overall strongest effect, but this comes with high uncertainty. Taken alone, the influence of quality, filtering, and software comes with huge uncertainty. This makes sense as the influence of software largely depends on the depth map generation quality, of which we are fairly certain. Filtering mode has the overall weakest effect on tree detection and this comes with relatively high certainty, especially conditional on the depth map generation quality.

and perform model selection via information criteria with the `brms::loo_compare()` function

```{r}
brms_f_mod5 = brms::add_criterion(brms_f_mod5, criterion = c("loo", "waic"))
brms::loo_compare(brms_f_mod1, brms_f_mod2, brms_f_mod3, brms_f_mod4, brms_f_mod5, criterion = "loo")
# brms::model_weights(brms_f_mod1, brms_f_mod2, brms_f_mod3, brms_f_mod4) %>% round(3)
```

#### Additional Plots for Export

`patchwork` of F-score contrasts

```{r, fig.height=9, include=FALSE, eval=FALSE}
### old
layout_temp = c(
    # area(t, l, b, r)
    patchwork::area(2, 1, 2, 1)
    , patchwork::area(4, 1, 4, 1)
    , patchwork::area(2, 3, 4, 3)
  )
  # check the layout
  # plot(layout_temp)
############################
# patchwork for height
############################
  ptchwrk_qlty_sftwr_comp + 
    labs(subtitle = "A: Quality Contrast by Software") +
    theme(plot.background = element_rect(colour = "gray88", fill=NA, size=3)) +
  ptchwrk_fltr_sftwr_comp + 
    labs(subtitle = "B: Filtering Mode Contrast by Software") +
    theme(plot.background = element_rect(colour = "gray88", fill=NA, size=3)) +
  ptchwrk_sftwr_qlty_comp + 
    labs(subtitle = "C: Software Contrast by Quality") +
    theme(plot.background = element_rect(colour = "gray88", fill=NA, size=3)) +
  # plot_annotation(tag_levels = list(c('#', '&'), '1')) +
  patchwork::plot_layout(
    design = layout_temp
    , widths = c(1,0.01,0.8)
    , heights = c(0.01,1,0.01,1,0.01)
  ) &
  scale_x_continuous(
    limits = c(-0.8,1.1)
    , breaks = seq(-0.8,0.8,0.4)
    , labels = seq(-0.8,0.8,0.4) %>% scales::number(accuracy = 0.1)
  ) &
  theme(
    axis.title.y = element_blank()
    , plot.subtitle = element_text(face = "bold", hjust = 0.5)
    # , plot.background = element_rect(colour = "gray88", fill=NA, size=3)
  )
ggplot2::ggsave(
    filename = paste0("../data/all_fscore_contrasts.jpeg")
    , plot = ggplot2::last_plot()
    , width = 11
    , height = 8.5
    , units = "in"
    , dpi = "print"
  )
```

```{r, fig.height=9, include=T, eval=T}
layout_temp = c(
    # area(t, l, b, r)
    patchwork::area(2, 1, 2, 1)
    , patchwork::area(2, 3, 2, 3)
    , patchwork::area(4, 1, 4, 3)
  )
  # check the layout
  # plot(layout_temp)
############################
# patchwork for height
############################
  ptchwrk_qlty_sftwr_comp + 
    labs(subtitle = "A: Quality Contrast by Software") +
    theme(plot.background = element_rect(colour = "gray88", fill=NA, size=3)) +
  ptchwrk_fltr_sftwr_comp + 
    labs(subtitle = "B: Filtering Mode Contrast by Software") +
    theme(plot.background = element_rect(colour = "gray88", fill=NA, size=3)) +
  patchwork::free(
    ptchwrk_sftwr_qlty_comp + 
    labs(subtitle = "C: Software Contrast by Quality") +
    theme(plot.background = element_rect(colour = "gray88", fill=NA, size=3)) ) +
  # plot_annotation(tag_levels = list(c('#', '&'), '1')) +
  patchwork::plot_layout(
    design = layout_temp
    , widths = c(1,0.01,1)
    , heights = c(0.01,1,0.01,1,0.01)
  ) &
  scale_x_continuous(
    limits = c(-0.58,0.96)
    , breaks = seq(-0.8,0.8,0.4)
    , labels = seq(-0.8,0.8,0.4) %>% scales::number(accuracy = 0.1)
  ) &
  theme(
    axis.title.y = element_blank()
    , plot.subtitle = element_text(face = "bold", hjust = 0.0)
    # , plot.background = element_rect(colour = "gray88", fill=NA, size=3)
  )
ggplot2::ggsave(
    filename = paste0("../data/all_fscore_contrasts.jpeg")
    , plot = ggplot2::last_plot()
    , width = 11
    , height = 8.5
    , units = "in"
    , dpi = "print"
  )
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
remove(list = ls()[grep("ptchwrk_",ls())])
gc()
```

## Heterogeneous variances and robustness against outliers

In our previous model of the data $y_i$ described by a beta distribution with [three nominal predictors and site effects](#beta_mod) our [posterior predictive checks](#ppcheck_mod5) indicated potential improvements for our model fit.

[Kruschke (2015)](https://sites.google.com/site/doingbayesiandataanalysis/) notes: 

>As was mentioned earlier in the chapter, we have assumed normally distributed data within groups, and equal variances across the groups, merely for simplicity and for consistency with traditional ANOVA. We can relax those assumptions in Bayesian software. In this section, we use t distributed noise instead of normal distributions, and we provide every group with its own standard-deviation parameter. Moreover, we put a hierarchical prior on the standard-deviation parameters, so that each group mutually informs the standard deviations of the other groups via the higher-level distribution...Instead of there being a single $\sigma_y$ parameter that applies to all groups, each group has its own scale parameter, $\sigma_j$. (p. 573-574)

and see section 20 from [Kurz's ebook supplement](https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-nominal-predictors.html#heterogeneous-variances-and-robustness-against-outliers-1)

### Bayesian{#f_5_5_pred_mod_bays}

With the beta likelihood our model with three nominal predictor variables, subject-level effects, and heterogeneous variances and robustness against outliers the model becomes:

\begin{align*}
y_{i} \sim & \operatorname{Beta} \bigl(\mu_{i}, , \phi_{[jkf](i)} \bigr) \\
\operatorname{logit}(\mu_{i}) = & \beta_0 \\
& + \sum_{j} \beta_{1[j]} x_{1[j]} + \sum_{k} \beta_{2[k]} x_{2[k]} + \sum_{f} \beta_{3[f]} x_{3[f]} + \sum_{s} \beta_{4[s]} x_{4[s]}  \\
& + \sum_{j,k} \beta_{1\times2[j,k]} x_{1\times2[j,k]} + \sum_{j,f} \beta_{1\times3[j,f]} x_{1\times3[j,f]} + \sum_{k,f} \beta_{2\times3[k,f]} x_{2\times3[k,f]} \\
& + \sum_{j,k,f} \beta_{1\times2\times3[j,k,f]} x_{1\times2\times3[j,k,f]} \\
\beta_{0}  \sim & \operatorname{Normal}(0,1) \\ 
\beta_{1[j]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1}}) \\ 
\beta_{2[k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2}}) \\ 
\beta_{3[f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{3}}) \\ 
\beta_{4[s]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{4}}) \\ 
\beta_{1\times2[j,k]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times2}}) \\ 
\beta_{1\times3[j,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{1\times3}}) \\ 
\beta_{2\times3[k,f]}  \sim & \operatorname{Normal}(0,\sigma_{\beta_{2\times3}}) \\ 
\sigma_{\beta_{1}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{2}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{4}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{1\times2}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{1\times3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\sigma_{\beta_{2\times3}} \sim & \operatorname{Gamma}(1.28,0.005) \\ 
\phi_{[jkf](i)} \sim & \operatorname{Gamma}(\alpha,\theta) \\
\alpha \sim & {\sf XXX} (xxx,xxx) \\
\theta \sim & {\sf XXX} (xxx,xxx)
\end{align*}

*!!!! Need to check how our $\phi_{[jkf](i)}$ is distributed*

, where $j$ is the depth map generation quality setting corresponding to observation $i$, $k$ is the depth map filtering mode setting corresponding to observation $i$, $f$ is the processing software corresponding to observation $i$, and $s$ is the study site corresponding to observation $i$

`brms` allows us to model the precision ($\phi$) but it is not required. If $\phi$ is not modeled, you still get a precision component, but it is universal across all the different coefficients (it doesn’t vary across any variables in the model). [Heiss](https://www.andrewheiss.com/blog/2021/11/08/beta-regression-guide/#b-beta-regression-bayesian-style) explains that:

>for whatever mathy reasons, when you don’t explicitly model the precision, the resulting coefficient in the table isn’t on the log scale—it’s a regular non-logged number, so there’s no need to exponentiate.

in thie `brms` [community post](https://discourse.mc-stan.org/t/understanding-parameters-of-beta-family-in-brms/21640/8) it is similarly noted that:

>If you don’t predict the parameters, you give priors for them on non-transformed scale. When you predict them, the predictors become linear coefficients as any other and work on the transformed scale - the transformations are specified by the link_XX parameters of the families (and you can change them if you need).

*Need to check our prior selection...just go with `brms` defaults for now*

Now fit the model.

```{r}
brms_f_mod5_5 = brms::brm(
  formula = brms::bf(
  ### model for y
    f_score ~ 
    # baseline
    1 + 
    # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    (1 | study_site) + # only fitting main effects of site and not interactions
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
  ### model for phi
    , phi  ~ 1 + (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
  )
  , data = ptcld_validation_data
  , family = Beta(link = "logit")
  , iter = 20000, warmup = 10000, chains = 4
  , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = round(parallel::detectCores()/2)
  , file = paste0(rootdir, "/fits/brms_f_mod5_5")
)
# brms::make_stancode(brms_f_mod5_5)
# print(brms_f_mod5_5)
# brms::prior_summary(brms_f_mod5_5)
```

```{r, include=F, eval=F}
# bayesian p-value
# http://www.stat.columbia.edu/~gelman/research/published/STS149A.pdf
# https://mc-stan.org/docs/stan-users-guide/posterior-predictive-checks.html

# It is the probability that a test statistic in the reference distribution exceeds its value in the data [p.value(y) = Pr(T(y.rep) > T(y))].

# Bayesian P values for mean and standard deviation test statistics The P values for the mean (P mean) give the probability that the mean of the data of new, out-of-sample observations simulated from the model exceeds the mean of the observed data. The P values for the standard deviation (P SD) give the probability that the standard deviation of new, out-of-sample observations simulated from the model exceeds the standaard deviation of the observed data. Large (> 0.90) or small (< 0.10) values indicate lack of fit.

brms::pp_check(brms_f_mod1, type = "stat_2d")
brms::pp_check(brms_f_mod1, type = "stat", stat = "sd")
brms::pp_check(brms_f_mod1, type = "stat", stat = "mean")
# try it ourself!
tidybayes::get_variables(brms_f_mod5_5)
# get draws from the posterior predictive distribution
brms::posterior_predict(brms_f_mod5_5, ndraws = 1000) %>% 
  dplyr::as_tibble() %>% 
  dplyr::mutate(draw = dplyr::row_number()) %>% 
  tidyr::pivot_longer(cols = -draw, values_to = "y_rep") %>% 
  dplyr::mutate(y_n = readr::parse_number(name)) %>% 
  # join with original data
  dplyr::inner_join(
    ptcld_validation_data %>% 
      dplyr::select(f_score) %>% 
      dplyr::rename(y=f_score) %>% 
      dplyr::mutate(y_n = dplyr::row_number())
    , by = dplyr::join_by("y_n")
  ) %>% 
  dplyr::select(-c(y_n)) %>% 
  dplyr::group_by(draw) %>% 
  # make test statistic
  dplyr::summarise(
    # test statistics y
    mean_y = mean(y)
    , sd_y = sd(y)
    # test statistics y_sim
    , mean_y_rep = mean(y_rep)
    , sd_y_rep = sd(y_rep)
  ) %>% 
  # summary()
  # ggplot() + geom_density(aes(x = sd_y_rep)) + geom_vline(xintercept = sd(ptcld_validation_data$f_score), lwd = 2)
  # p-values
  dplyr::ungroup() %>% 
  dplyr::mutate(
    p_val_mean = as.numeric(mean_y_rep > mean_y)
    , p_val_sd = as.numeric(sd_y_rep > sd_y)
  ) %>% 
  dplyr::select(draw, tidyselect::starts_with("p_val_")) %>% 
  tidyr::pivot_longer(tidyselect::starts_with("p_val_"), names_prefix = "p_val_") %>% 
  dplyr::group_by(name) %>% 
  # summarize p-vals
  dplyr::summarise(
    mean = mean(value)
    , sd = sd(value) # not sure why this sd doesn't match what's shown in the brms::pp_check??????
    , `2.5%` = quantile(value, probs = 0.025)
    , `50%` = quantile(value, probs = 0.50)
    , `97.5%` = quantile(value, probs = 1-0.025)
  )

```


#### Posterior Predictive Checks

Markov chain Monte Carlo (MCMC) simulations were conducted using the `brms` package (Bürkner 2017) to estimate posterior predictive distributions of the parameters of interest. We ran three chains of 100,000 iterations with the first 50,000 discarded as burn-in. Trace-plots were utilized to visually assess model convergence and sufficient convergence was checked with $\hat{R}$ values near 1 ([Brooks & Gelman, 1998](https://scholar.google.com/scholar?cluster=14209404114665352991&hl=en&as_sdt=0,6)). Posterior predictive checks were used to evaluate model goodness-of-fit by comparing data simulated from the model with the observed data used to estimate the model parameters ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)). Calculating the proportion of MCMC iterations in which the test statistic (i.e., mean and sum of squares) from the simulated data and observed data are more extreme than one another provides the Bayesian P-value. Lack of fit is indicated by a value close to 0 or 1 while a value of 0.5 indicates perfect fit ([Hobbs & Hooten, 2015](https://scholar.google.com/scholar?cluster=9228589188684720156&hl=en&as_sdt=0,6)).

To learn more about this approach to posterior predictive checks, check out Gabry’s (2022) vignette, [Graphical posterior predictive checks using the bayesplot package](https://cran.r-project.org/web/packages/bayesplot/vignettes/graphical-ppcs.html). 

check the trace plots for problems with convergence of the Markov chains

```{r, fig.height=8}
plot(brms_f_mod5_5)
```

and check our $\hat{R}$ values

```{r}
brms_f_mod5_5 %>% 
  brms::rhat() %>% 
  as.data.frame() %>% 
  tibble::rownames_to_column(var = "parameter") %>%
  dplyr::rename_with(tolower) %>% 
  dplyr::rename(rhat = 2) %>% 
  dplyr::filter(
    stringr::str_starts(parameter, "b_") 
    | stringr::str_starts(parameter, "r_") 
    | stringr::str_starts(parameter, "sd_") 
    | parameter == "phi"
  ) %>%
  dplyr::mutate(
    parameter = parameter %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
    , chk = (rhat <= 1*0.998 | rhat >= 1*1.002)
  ) %>% 
  ggplot(aes(x = rhat, y = parameter, color = chk, fill = chk)) +
  geom_vline(xintercept = 1, linetype = "dashed", color = "gray44", lwd = 1.2) +
  geom_vline(xintercept = 1*0.998, lwd = 1.5) +
  geom_vline(xintercept = 1*1.002, lwd = 1.5) +
  geom_vline(xintercept = 1*0.999, lwd = 1.2, color = "gray33") +
  geom_vline(xintercept = 1*1.001, lwd = 1.2, color = "gray33") +
  geom_point() +
  scale_fill_manual(values = c("navy", "firebrick")) +
  scale_color_manual(values = c("navy", "firebrick")) +
  labs(
    x = latex2exp::TeX("$\\hat{R}$")
    , subtitle = latex2exp::TeX("posterior-predictive check for parameter $\\hat{R}$ estimates")
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , axis.text.y = element_text(size = 4)
    , panel.grid.major.x = element_blank()
    , panel.grid.minor.x = element_blank()
  )
```

posterior-predictive check to make sure the model does an okay job simulating data that resemble the sample data

```{r}
# posterior predictive check
brms::pp_check(
    brms_f_mod5_5
    , type = "dens_overlay"
    , ndraws = 100
  ) + 
  labs(subtitle = "posterior-predictive check (overlaid densities)") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )

ggplot2::ggsave("../data/ppchk_ovrll_mod5_5.png", height = 7, width = 10.5)
```

another way

```{r}
brms::pp_check(brms_f_mod5_5, type = "ecdf_overlay", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (ECDF: empirical cumulative distribution function)") + 
  theme_light() +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

and another posterior predictive check for the overall model

```{r}
# means
p1_temp = brms::pp_check(
    brms_f_mod5_5
    , type = "stat"
    , stat = "mean"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  theme_light()
# sds
p2_temp = brms::pp_check(
    brms_f_mod5_5
    , type = "stat"
    , stat = "sd"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "sd's") +
  theme_light()

# combine 
(p1_temp + p2_temp) &
  theme(legend.position = "none") &
  plot_annotation(
    title = "Posterior-predictive statistical checks\noverall model"
    , subtitle = expression(
      "The dark blue lines are "*italic(T(y))*", and the light blue bars are for "*italic(T)(italic(y)[rep])*".")
  )
```

and another posterior predictive check for the overall model combining mean and sd

```{r}
brms::pp_check(brms_f_mod5_5, type = "stat_2d") +
  theme_light() +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

How’d we do capturing the conditional means and standard deviations by depth map generation quality?

```{r}
# means
p1_temp = brms::pp_check(
    brms_f_mod5_5
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "mean"
    , group = "depth_maps_generation_quality"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  facet_grid(cols = vars(group), scales = "free") +
  theme_light()
# sds
p2_temp = brms::pp_check(
    brms_f_mod5_5
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "sd"
    , group = "depth_maps_generation_quality"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "sd's") +
  facet_grid(cols = vars(group), scales = "free") +
  theme_light()
# combine 
(p1_temp / p2_temp) &
  theme(legend.position = "none") &
  plot_annotation(
    title = "Posterior-predictive statistical checks\nby dense cloud quality"
    , subtitle = expression(
      "The dark blue lines are "*italic(T(y))*", and the light blue bars are for "*italic(T)(italic(y)[rep])*".")
  )

ggplot2::ggsave("../data/ppchk_qlty_mod5_5.png", height = 7, width = 10.5)
```

Both the means and sd's of the F-score are well represented across the different levels of dense cloud quality

What about for the software?

```{r}
pp_check(brms_f_mod5_5, "dens_overlay_grouped", group = "software", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (overlaid densities)\nby software") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )

ggplot2::ggsave("../data/ppchk_sftwr_mod5_5.png", height = 7, width = 10.5)
```

and what about for the filtering mode?

```{r}
pp_check(brms_f_mod5_5, "dens_overlay_grouped", group = "depth_maps_generation_filtering_mode", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (overlaid densities)\nby filtering mode") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

We didn't see much change in our posterior predictive checks compared to our previous model of the data $y_i$ described by a beta distribution with [three nominal predictors and site effects](#beta_mod) our [posterior predictive checks](#ppcheck_mod5).

The Bayesian p-value is the probability that a test statistic in the reference distribution exceeds its value in the data. The Bayesian p-value is calculated from the posterior predictive distribution of the new data and the distribution of the observed data. We estimate the probability that the test statistic calculated from "new" data arising from our model ($y_{new}$) is more extreme than the test statistic calculated from the observed data ($y$): $\text{P-value}(y) = Pr(T(y_{new}) > T(y))$ where the test statistic $T(y)$ describes the distribution of the data as a summary of the data; it could be the mean, variance, the coefﬁcient of variation, the kurtosis, the maximum, or the minimum of the observed data set, or it might be an "omnibus" statistic like a squared discrepancy or a chi-square value [Hobbs and Hooten (2015, p. 188)](https://scholar.google.com/scholar?cluster=9228589188684720156&oi=gsb&hl=en&as_sdt=0,6)

>Bayesian P values for mean and standard deviation test statistics The P values for the mean (P mean) give the probability that the mean of the data of new, out-of-sample observations simulated from the model exceeds the mean of the observed data. The P values for the standard deviation (P SD) give the probability that the standard deviation of new, out-of-sample observations simulated from the model exceeds the standard deviation of the observed data. Large ($\gtrapprox 0.90$) or small ($\lessapprox 0.10$) values indicate lack of fit. [Hobbs and Hooten (2015)](https://scholar.google.com/scholar?cluster=9228589188684720156&oi=gsb&hl=en&as_sdt=0,6);[Hobbs et al. (2024, Appendix S2 p. 8)](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1002/ecm.1598)

Check the Bayesian p-values between the models

```{r}
# bayesian p-value
get_mod_p_val = function(my_mod, ndraws = 1000){
  # get draws from the posterior predictive distribution
  brms::posterior_predict(my_mod, ndraws = ndraws) %>% 
    dplyr::as_tibble() %>% 
    dplyr::mutate(draw = dplyr::row_number()) %>% 
    tidyr::pivot_longer(cols = -draw, values_to = "y_rep") %>% 
    dplyr::mutate(y_n = readr::parse_number(name)) %>% 
    dplyr::select(-c(y_n)) %>% 
    dplyr::group_by(draw) %>% 
    # make test statistic
    dplyr::summarise(
      # test statistics y_sim
      mean_y_rep = mean(y_rep)
      , sd_y_rep = sd(y_rep)
    ) %>% 
    # # observed data test statistics
    dplyr::mutate(
    # test statistics y
      mean_y = mean(my_mod$data[,1])
      , sd_y = sd(my_mod$data[,1])
    ) %>% 
    # p-values
    dplyr::ungroup() %>% 
    dplyr::mutate(
      p_val_mean = as.numeric(mean_y_rep > mean_y)
      , p_val_sd = as.numeric(sd_y_rep > sd_y)
    ) %>% 
    # summarize p-vals
    dplyr::summarise(
      P.mean = mean(p_val_mean)
      , P.sd = mean(p_val_sd)
    )
}
# get the model p-values
set.seed(16) # replicate results
dplyr::bind_rows(
    get_mod_p_val(brms_f_mod4, ndraws = 5000) %>% 
      dplyr::mutate(model = "brms_f_mod4")
    , get_mod_p_val(brms_f_mod5, ndraws = 5000) %>% 
      dplyr::mutate(model = "brms_f_mod5")
    , get_mod_p_val(brms_f_mod5_5, ndraws = 5000) %>% 
      dplyr::mutate(model = "brms_f_mod5_5")
  ) %>% 
  dplyr::relocate(model) %>% 
  kableExtra::kbl(digits = 3) %>% 
  kableExtra::kable_styling()
```

perhaps we should use `brms_f_mod4` in which we [modeled F-score presuming a Gaussian likelihood](#f_4_pred_mod_bays)??? What are the pros and cons? Surely, an accurate representation of the $y_i$ data (as represented by the beta distribution) is meaningful even if "worse" p-values are found.

let's perform model selection via information criteria with the `brms::loo_compare()` function to check

```{r}
brms_f_mod5_5 = brms::add_criterion(brms_f_mod5_5, criterion = c("loo", "waic"))
brms::loo_compare(brms_f_mod1, brms_f_mod2, brms_f_mod3, brms_f_mod4, brms_f_mod5, brms_f_mod5_5, criterion = "loo")
# brms::model_weights(brms_f_mod1, brms_f_mod2, brms_f_mod3, brms_f_mod4) %>% round(3)
```

The information criteria suggests that we continue with `brms_f_mod5` in which $y_i$ is described by a beta distribution with [three nominal predictors and site effects](#beta_mod) and we did not model dispersion, instead using a single $\phi_y$ parameter that applies to all groups

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

## Overstory and Understory Validation

In our [validation data creation process](#field_valid) we calculated F-score for overstory and understory trees separately.

Here, let's add a factor variable with levels for overstory and understory as a predictor to our [final model from above](#beta_mod). 

The overall F-score is not the same as combining the overstory and understory F-score by calculating the mean of these values due to the differing number of observations in each. To build this model we need to convert our data to long format so that a row is also unique by our overstory/understory factor.

Each study site contributes one observation per dense cloud quality, filtering mode, software, and overstory/understory factor. That is, a row in the underlying data is unique by study site, software, dense cloud quality, filtering mode, and overstory/understory factor.

### Summary Statistics

```{r}
ou_ptcld_validation_data =
  ptcld_validation_data %>% 
  dplyr::select(
    study_site, software
    , depth_maps_generation_quality
    , depth_maps_generation_filtering_mode
    # our dependent var in wide format
    , understory_f_score, overstory_f_score
  ) %>% 
  tidyr::pivot_longer(
    cols = tidyselect::ends_with("_f_score")
    , names_to = "story"
    , values_to = "f_score"
    , values_drop_na = F
  ) %>% 
  dplyr::mutate(
    story = story %>% stringr::str_remove_all("_f_score") %>% factor()
  )
# what is this data?
ou_ptcld_validation_data %>% dplyr::glimpse()
# a row is unique by...
identical(
  nrow(ou_ptcld_validation_data)
  , ou_ptcld_validation_data %>% 
    dplyr::distinct(
      study_site, software
      , depth_maps_generation_quality
      , depth_maps_generation_filtering_mode
      , story
    ) %>% 
    nrow()
)
# and we should have 2 times the rows as the orignial data
identical(
  nrow(ou_ptcld_validation_data) %>% as.numeric() # long data
  , nrow(ptcld_validation_data)*2 %>% as.numeric() # original wide data
)
```

quick summary stats

```{r}
ptcld_validation_data %>% 
  dplyr::select(tidyselect::ends_with("_f_score")) %>% 
  summary()
```

let's check this data with our `geom_tile` plot

```{r, fig.height=12}
# create function for combining plots with patchwork
plt_tile_fn_temp = function(ss = "overstory"){
  ou_ptcld_validation_data %>% 
    dplyr::filter(tolower(story) == tolower(ss)) %>% 
    dplyr::group_by(story, software, depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>%
    # collapse across study site
    dplyr::summarise(
      mean_f_score = mean(f_score, na.rm = T)
      # , med_f_score = median(f_score, na.rm = T)
      , sd_f_score = sd(f_score, na.rm = T)
      , n = dplyr::n()
    ) %>% 
    ggplot(mapping = aes(
      y = depth_maps_generation_quality
      , x = depth_maps_generation_filtering_mode
      , fill = mean_f_score
      , label = paste0(scales::comma(mean_f_score,accuracy = 0.01), "\n(n=", n,")")
    )) +
    geom_tile(color = "white") +
    geom_text(color = "white", size = 3) +
    facet_grid(cols = vars(software)) + 
    scale_x_discrete(expand = c(0, 0)) +
    scale_y_discrete(expand = c(0, 0)) +
    scale_fill_viridis_c(
      option = "cividis", begin = 0.3, end = 0.9
      , limits = c(0,0.75)
    ) +
    labs(
      x = "filtering mode"
      , y = "quality"
      , fill = "F-score"
      , subtitle = "mean F-score and # of study sites"
      , title = toupper(ss)
    ) +
    theme_light() + 
    theme(
      legend.position = "none"
      , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
      , panel.background = element_blank()
      , panel.grid = element_blank()
      , plot.subtitle = element_text(hjust = 0.5)
      , plot.title = element_text(face = "bold")
      , strip.text = element_text(color = "black", face = "bold")
    )
}
plt_tile_fn_temp("overstory") / plt_tile_fn_temp("understory")
```

now let's check by our other predictor variables

```{r}
sum_stats_dta = function(my_var){
  sum_fns = list(
    n = ~sum(ifelse(is.na(.x), 0, 1))  
    , min = ~min(.x, na.rm = TRUE)
    , max = ~max(.x, na.rm = TRUE)
    , mean = ~mean(.x, na.rm = TRUE)
    , median = ~median(.x, na.rm = TRUE)
    , sd = ~sd(.x, na.rm = TRUE)
  )
  # plot
  (
    ggplot(
      data = ou_ptcld_validation_data %>% 
        dplyr::group_by(.data[[my_var]]) %>%
        dplyr::mutate(m = median(f_score))
      , mapping = aes(
        y = .data[[my_var]]
        , x = f_score, fill = m)
      ) +
      geom_violin(color = NA) + 
      geom_boxplot(width = 0.1, outlier.shape = NA, fill = NA, color = "black") +
      geom_rug() +
      facet_grid(cols = vars(story)) +
      scale_fill_viridis_c(option = "mako", begin = 0.3, end = 0.9, direction = -1, limits = c(0,0.75)) +
      labs(
        x = "F-score"
        , y = stringr::str_replace_all(my_var, pattern = "_", replacement = " ")
        , subtitle = stringr::str_replace_all(my_var, pattern = "_", replacement = " ") %>% 
          stringr::str_to_title()
      ) +
      theme_light() +
      theme(legend.position = "none", strip.text = element_text(color = "black", face = "bold"))
  )
  
}
# sum_stats_dta("software")
```

summarize for all variables of interest

```{r}
c("software", "study_site"
  , "depth_maps_generation_quality"
  , "depth_maps_generation_filtering_mode"
) %>% 
  purrr::map(sum_stats_dta)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```

### Bayesian{#f_6_pred_mod_bays}

*Need to check our prior selection...just go with `brms` defaults for now*

Now fit the model.

```{r}
brms_f_mod6 = brms::brm(
  formula = f_score ~ 
    # baseline
    1 + 
    # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    (1 | story) +
    (1 | study_site) + # only fitting main effects of site and not interactions
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    (1 | depth_maps_generation_quality:story) +
    (1 | depth_maps_generation_filtering_mode:story) +
    (1 | software:story) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software) +
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:story) +
    (1 | depth_maps_generation_quality:software:story) +
    (1 | depth_maps_generation_filtering_mode:software:story) +
    # four-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software:story)
  , data = ou_ptcld_validation_data
  , family = Beta(link = "logit")
  , iter = 20000, warmup = 10000, chains = 4
  , control = list(adapt_delta = 0.999, max_treedepth = 13)
  , cores = round(parallel::detectCores()/2)
  , file = paste0(rootdir, "/fits/brms_f_mod6")
)
```

check the trace plots for problems with convergence of the Markov chains

```{r, fig.height=8}
plot(brms_f_mod6)
```

posterior-predictive check to make sure the model does an okay job simulating data that resemble the sample data

```{r}
# posterior predictive check
brms::pp_check(
    brms_f_mod6
    , type = "dens_overlay"
    , ndraws = 100
  ) + 
  labs(subtitle = "posterior-predictive check (overlaid densities)") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

How’d we do capturing the conditional means and standard deviations by depth map generation quality?

```{r}
# means
p1_temp = brms::pp_check(
    brms_f_mod6
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "mean"
    , group = "depth_maps_generation_quality"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "means") +
  facet_grid(cols = vars(group), scales = "free") +
  theme_light()
# sds
p2_temp = brms::pp_check(
    brms_f_mod6
    , type = "stat_grouped" # "dens_overlay_grouped"
    , stat = "sd"
    , group = "depth_maps_generation_quality"
  ) + 
  scale_y_continuous(NULL, breaks = c(NULL)) +
  labs(subtitle = "sd's") +
  facet_grid(cols = vars(group), scales = "free") +
  theme_light()
# combine 
(p1_temp / p2_temp) &
  theme(legend.position = "none") &
  plot_annotation(
    title = "Posterior-predictive statistical checks\nby dense cloud quality"
    , subtitle = expression(
      "The dark blue lines are "*italic(T(y))*", and the light blue bars are for "*italic(T)(italic(y)[rep])*".")
  )

# brms_f_mod6 %>% 
#   brms::posterior_summary()
```

The means and sd's of the F-score are less well represented across the different levels of dense cloud quality. We may want to look into modelling heterogeneous variances as shown by [Kruschke](https://sites.google.com/site/doingbayesiandataanalysis/) chapter 20 and [Kurz](https://bookdown.org/content/3686/metric-predicted-variable-with-multiple-nominal-predictors.html#heterogeneous-variances-and-robustness-against-outliers-1)

What about for the software?

```{r}
pp_check(brms_f_mod6, "dens_overlay_grouped", group = "software", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (overlaid densities)\nby software") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

What about for the overstory/understory?

```{r}
pp_check(brms_f_mod6, "dens_overlay_grouped", group = "story", ndraws = 100) +
  labs(subtitle = "posterior-predictive check (overlaid densities)\nby story") +
  theme_light() +
  scale_y_continuous(NULL, breaks = NULL) +
  theme(
    legend.position = "top", legend.direction = "horizontal"
    , legend.text = element_text(size = 14)
  )
```

We may not be capturing the high versus low tree predictive performance that is being driven by differences at the study site level. 

*this should be addressed...is validation really that bad for some study sites?*

We can look at the model noise standard deviation (concentration) $\phi$. 

```{r}
# get formula
form_temp = brms_f_mod6$formula$formula[3] %>% 
  as.character() %>% get_frmla_text(split_chrs = 115) %>% 
  stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
  stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering")
# extract the posterior draws
brms::as_draws_df(brms_f_mod6) %>% 
# plot
  ggplot(aes(x = phi, y = 0)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21, point_size = 3
    , quantiles = 100
  ) +
  scale_y_continuous(NULL, breaks = NULL) +
  labs(
    x = latex2exp::TeX("$\\phi$")
    , caption = form_temp
  ) +
  theme_light()
```

we can compare this to our beta model above without any overstory/understory predictor variable.

```{r}
dplyr::bind_rows(
  brms::as_draws_df(brms_f_mod6) %>% dplyr::select(phi) %>% dplyr::mutate(src = "brms_f_mod6")
  , brms::as_draws_df(brms_f_mod5) %>% dplyr::select(phi) %>% dplyr::mutate(src = "brms_f_mod5")
) %>% 
  ggplot(mapping = aes(y = src, x = phi, color = src, group = src)) +
    tidybayes::stat_pointinterval(position = "dodge") +
    scale_y_discrete(NULL, breaks = NULL) +
    scale_color_manual(values = viridis::turbo(n = 6, begin = 0.2, end = 0.8)[5:6]) +
    labs(
      y = "", x = latex2exp::TeX("$\\phi$")
      , color = "model"
    ) +
    theme_light() +
    theme(legend.position = "top", strip.text = element_text(color = "black", face = "bold"))
```

This indicates that our model including overstory/understory has less dispersion

#### Story - main effect

Now, we'll look at the main effect of overstory/understory on F-score by collapsing across the filtering mode, dense cloud quality setting, study site, and software and then performing a pairwise comparison between overstory and understory

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ou_ptcld_validation_data %>%
  dplyr::distinct(story) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | story)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = story
      , fill = story
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "viridis", begin = 0.3, end = 0.7, drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

we can view the posterior HDI of these estimates

```{r}
ou_ptcld_validation_data %>%
  dplyr::distinct(story) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | story)
  ) %>% 
  dplyr::rename(value = .epred) %>%  
  dplyr::group_by(story) %>% 
  tidybayes::median_hdi(value) %>% 
  select(-c(.point,.interval, .width)) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of F-score"
    , col.names = c(
      ""
      , "F-score"
      , "HDI low", "HDI high"
    )
  ) %>% 
  kableExtra::kable_styling()
```

compare this to the quick summary stats

```{r}
ou_ptcld_validation_data %>% 
  dplyr::group_by(story) %>% 
  dplyr::summarise(
    mean_f = mean(f_score)
    , f_05 = quantile(f_score, probs = 0.05)
    , f_95 = quantile(f_score, probs = 0.95)
  ) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "summary statistics of F-score"
    , col.names = c(
      ""
      , "mean F-score"
      , "5% value", "95% value"
    )
  ) %>% 
  kableExtra::kable_styling()
```

now we'll make our contrast of overstory/understory

```{r}
brms_contrast_temp =
  ou_ptcld_validation_data %>%
    dplyr::distinct(story) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod6, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | story)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = story
      , comparison = tidybayes::emmeans_comparison("revpairwise")
    ) %>% 
    dplyr::rename(contrast = story) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "story"
)
ggplot2::ggsave(
  "../data/stry_comp_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast) %>% 
  dplyr::select(contrast, value, .lower, .upper, pr_gt_zero) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "overstory contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()

```

Averaging across all levels of filtering mode, dense cloud quality setting, study site, and software the process does a better job detecting overstory trees than understory trees. Because this is a Bayesian analysis we can quantify this probability: there is a `r brms_contrast_temp$pr_diff[1] %>% scales::percent(accuracy = 0.1)` that overstory trees will be detected with better accuracy than understory trees (see coloring on posterior predictive distribution above)

#### Story:Software - interaction

Now, we'll address the question of "are there differences in F-score based on the processing software used for detecting overstory and understory trees?"

```{r}
# get draws
stry_sftwr_draws_temp = ou_ptcld_validation_data %>%
  dplyr::distinct(software, story) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | story) +
      (1 | software) + 
      (1 | software:story)
  ) %>% 
  dplyr::rename(value = .epred)
# calculate contrast
brms_contrast_temp = stry_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = software
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = software) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, story) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "software"
  , facet = "story"
  , label_size = 2.5
  , x_expand = c(0.17,0.3)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby overstory/understory"
  )

ggplot2::ggsave(
  "../data/sftwr_stry_comp_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, story, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, story) %>% 
  dplyr::select(contrast, story, value, .lower, .upper, pr_gt_zero) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "software contrast"
      , "overstory/understory"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()
```

#### Story:Quality - interaction

What is the difference in F-score between overstory and understory for the different levels of dense cloud quality?

Note that how within `tidybayes::add_epred_draws`, we used the `re_formula` argument to average over the effects of study site, software, and filtering mode to compare the dense cloud quality and story effects.

```{r}
ou_ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality, story) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | story) + 
      (1 | depth_maps_generation_quality:story)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = story
      , fill = story
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.8
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  scale_fill_viridis_d(option = "viridis", begin = 0.3, end = 0.7, drop = F) +
  scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
  facet_grid(cols = vars(depth_maps_generation_quality)) +
  labs(
    x = "story", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
    , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
  ) 
```

#### Quality:Filtering - interaction

Are there differences in F-score based on dense point cloud generation quality within each level of filtering mode?

Here, we collapse across the study site, software, and story to compare the combined dense cloud quality and filtering mode effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
qlty_filter_draws_temp = 
  ptcld_validation_data %>% 
    dplyr::distinct(depth_maps_generation_quality, depth_maps_generation_filtering_mode) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod6, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_quality) +
        (1 | depth_maps_generation_filtering_mode) + 
        (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode)
    ) %>% 
    dplyr::rename(value = .epred)
# plot
  qlty_filter_draws_temp %>% 
    dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_eye(
      point_interval = median_hdi, .width = .95
      , slab_alpha = 0.8
      , interval_color = "black", linewidth = 1
      , point_color = "black", point_fill = "black", point_size = 1
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
    facet_grid(cols = vars(depth_maps_generation_quality)) +
    labs(
      x = "filtering mode", y = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
      , fill = "Filtering Mode"
      , caption = form_temp
    ) +
    theme_light() +
    theme(
      legend.position = "none"
      , legend.direction  = "horizontal"
      , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
      , strip.text = element_text(color = "black", face = "bold")
    ) 
ggplot2::ggsave(
  "../data/qlty_fltr_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
brms_contrast_temp = qlty_filter_draws_temp %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = 
        contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
    , depth_maps_generation_filtering_mode = depth_maps_generation_filtering_mode %>% 
      factor(
        levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
        , ordered = T
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast,depth_maps_generation_filtering_mode) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "depth_maps_generation_filtering_mode"
  , label_size = 2.0
  , x_expand = c(0,0.6)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby filtering mode"
  ) +
  theme(
    axis.text.x = element_text(size = 7)
  )
ggplot2::ggsave(
  "../data/qlty_fltr_comp_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_filtering_mode, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_filtering_mode) %>% 
  dplyr::select(contrast, depth_maps_generation_filtering_mode, value, .lower, .upper, pr_gt_zero) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "filtering mode"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "8in")
```

#### Software:Quality - interaction

It might be more important to understand the difference in F-score by dense cloud quality and software rather than filtering mode since filtering mode had such a small effect on the SfM predictive ability

Are there differences in F-score based on dense point cloud generation quality within each different processing software? We will also address the similar but slightly different question of "are there differences in F-score based on the processing software used at a given dense point cloud generation quality?"

Here, we collapse across the study site, filtering mode, and story to compare the combined dense cloud quality and software effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality, software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | software) + 
      (1 | depth_maps_generation_quality:software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  # plot
  ggplot(
    mapping = aes(
      y = value, x = software
      , fill = software
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.8
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
  scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
  facet_grid(cols = vars(depth_maps_generation_quality)) +
  labs(
    x = "software", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality"
    , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
  ) 
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
# get draws
qlty_sftwr_draws_temp = 
  tidyr::crossing(
    depth_maps_generation_quality = unique(ptcld_validation_data$depth_maps_generation_quality)
    , software = unique(ptcld_validation_data$software)
  ) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) +
      (1 | software) + 
      (1 | depth_maps_generation_quality:software)
  ) %>% 
  dplyr::rename(value = .epred)

# calculate contrast
brms_contrast_temp = qlty_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_quality
    , comparison = contrast_list
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software) %>% 
  make_contrast_vars()

# remove out-of-sample obs
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(software, sorter1 == depth_maps_generation_quality)
  ) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(software, sorter2 == depth_maps_generation_quality)
  )
  
# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
  , facet = "software"
  , label_size = 2.2
  , x_expand = c(0.46,0.75)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby software"
  )

ggplot2::ggsave(
  "../data/qlty_sftwr_comp_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, software, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, software) %>% 
  dplyr::select(contrast, software, value, .lower, .upper, pr_gt_zero) %>% 
   
  dplyr::arrange(software, contrast) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "software"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```

The contrasts above address the question "are there differences in F-score based on dense point cloud generation quality within each software?".

To address the different question of "are there differences in F-score based on the processing software used at a given dense point cloud generation quality?" we need to utilize a different formulation of the `comparison` parameter within our call to the `tidybayes::compare_levels` function and calculate the contrast by `software` instead

```{r}
# calculate contrast
brms_contrast_temp = qlty_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = software
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = software) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, depth_maps_generation_quality) %>% 
  make_contrast_vars()

# remove out-of-sample obs
brms_contrast_temp = brms_contrast_temp %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(sorter1 == software, depth_maps_generation_quality)
  ) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality)
    , by = dplyr::join_by(sorter2 == software, depth_maps_generation_quality)
  )
# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "software"
  , facet = "depth_maps_generation_quality"
  , label_size = 2.0
  , x_expand = c(0.17,0.14)
) +
  facet_wrap(facets = vars(depth_maps_generation_quality), ncol = 2) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby dense cloud quality"
  ) +
  theme(
    legend.position = c(.75, .13)
  ) +
  guides(fill = guide_colorbar(theme = theme(
    legend.key.width  = unit(1, "lines"),
    legend.key.height = unit(7, "lines")
  )))

ggplot2::ggsave(
  "../data/sftwr_qlty_comp_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, depth_maps_generation_quality, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, depth_maps_generation_quality) %>% 
  dplyr::select(contrast, depth_maps_generation_quality, value, .lower, .upper, pr_gt_zero) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "software contrast"
      , "quality"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "8in")
```

#### Software:Filtering - interaction

Are there differences in F-score based on dense point cloud filtering mode within each processing software?

Here, we collapse across the study site, depth map generation quality, and story to compare the combined filtering mode and software effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

Even though filtering mode had a small effect on the SfM predictive ability when averaging across all softwares, there might still be differences in filtering mode within software when we average across all depth map generation quality settings. Let's check the difference in F-score by depth map filtering mode and software.

```{r}
# get draws
fltr_sftwr_draws_temp = ptcld_validation_data %>%
  dplyr::distinct(depth_maps_generation_filtering_mode, software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode) +
      (1 | software) + 
      (1 | depth_maps_generation_filtering_mode:software)
  ) %>% 
  dplyr::rename(value = .epred)

  # plot
fltr_sftwr_draws_temp %>% 
  ggplot(
    mapping = aes(
      y = value, x = software
      , fill = software
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.8
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
  scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
  facet_grid(cols = vars(depth_maps_generation_filtering_mode)) +
  labs(
    x = "software", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby filtering mode"
    , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "none"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
  ) 
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
# calculate contrast
brms_contrast_temp = fltr_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_filtering_mode
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_filtering_mode)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      ) %>% 
      # re order for filtering mode
      forcats::fct_rev()
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "filtering mode"
  , facet = "software"
  , label_size = 2.0
  , x_expand = c(0.85,0.8) # c(1,1.4)
) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby software"
  )

ggplot2::ggsave(
  "../data/fltr_sftwr_comp_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, software, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast, software) %>% 
  dplyr::select(contrast, software, value, .lower, .upper, pr_gt_zero) %>% 
   
  dplyr::arrange(software, contrast) %>% 
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "filtering contrast"
      , "software"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::scroll_box(height = "6in")
```

#### Software:Quality:Filtering - interaction

The contrasts immediately above address the question "are there differences in F-score based on dense point cloud filtering mode within each software?". Although the impact of filtering mode is small, it is highly probable when averaging across all quality settings. What if we don't average out the impact of quality and instead get the full, three-way interaction between software, quality, and filtering mode?

Let's get the model's answer to the question "For each software, are there differences in F-score based on dense point cloud filtering mode within each point cloud generation quality?".

Here, we collapse across the study site and story to compare the dense cloud quality, filtering mode, and software effect.

In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
# get draws
fltr_sftwr_draws_temp =
  tidyr::crossing(
    depth_maps_generation_quality = unique(ptcld_validation_data$depth_maps_generation_quality)
    , depth_maps_generation_filtering_mode = unique(ptcld_validation_data$depth_maps_generation_filtering_mode)
    , software = unique(ptcld_validation_data$software)
  ) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6, allow_new_levels = T
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality) + # main effects
    (1 | depth_maps_generation_quality) +
    (1 | depth_maps_generation_filtering_mode) + 
    (1 | software) +
    # two-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode) +
    (1 | depth_maps_generation_quality:software) +
    (1 | depth_maps_generation_filtering_mode:software) +
    # three-way interactions
    (1 | depth_maps_generation_quality:depth_maps_generation_filtering_mode:software)
  ) %>% 
  dplyr::rename(value = .epred)

# plot
fltr_sftwr_draws_temp %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
    , by = dplyr::join_by(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev()) %>% 
  ggplot(
    mapping = aes(
      y = value
      , x = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
  tidybayes::stat_eye(
    point_interval = median_hdi, .width = .95
    , slab_alpha = 0.8
    , interval_color = "black", linewidth = 1
    , point_color = "black", point_fill = "black", point_size = 1
  ) +
  scale_fill_viridis_d(option = "plasma", drop = F) +
  scale_y_continuous(breaks = scales::extended_breaks(n=10)) +
  facet_grid(
    rows = vars(software)
    , cols = vars(depth_maps_generation_quality)
    # , switch = "y"
  ) +
  labs(
    fill = "filtering mode"
    , x = "filtering mode", y = "F-score"
    , subtitle = "posterior predictive distribution of F-score with 95% HDI\nby dense cloud quality and software"
    # , caption = form_temp
  ) +
  theme_light() +
  theme(
    legend.position = "top"
    , legend.direction  = "horizontal"
    , axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)
    , strip.text = element_text(color = "black", face = "bold")
    # , strip.placement = "outside"
  ) +
  guides(
    fill = guide_legend(override.aes = list(shape = NA, size = 6, alpha = 0.9, lwd = NA))
  )

ggplot2::ggsave(
  "../data/qlty_fltr_sftwr_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

we can also make pairwise comparisons so long as we continue using `tidybayes::add_epred_draws` with the `re_formula` argument

```{r}
# calculate contrast
brms_contrast_temp = fltr_sftwr_draws_temp %>% 
  tidybayes::compare_levels(
    value
    , by = depth_maps_generation_filtering_mode
    , comparison = "pairwise"
  ) %>% 
  dplyr::rename(contrast = depth_maps_generation_filtering_mode)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
      "sorter"
      , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
    )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
          as.numeric()
      ) %>% 
      # re order for filtering mode
      forcats::fct_rev()
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast, software, depth_maps_generation_quality) %>% 
  make_contrast_vars()

# remove out-of-sample obs
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
    , by = dplyr::join_by(software, depth_maps_generation_quality, sorter1==depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::inner_join(
    ptcld_validation_data %>% dplyr::distinct(software, depth_maps_generation_quality, depth_maps_generation_filtering_mode)
    , by = dplyr::join_by(software, depth_maps_generation_quality, sorter2==depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::mutate(depth_maps_generation_quality = depth_maps_generation_quality %>% forcats::fct_rev())

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
brms_contrast_temp %>% 
  plt_contrast(
    facet = c("depth_maps_generation_quality", "software")
    , y_axis_title = "filtering mode"
    , label_size = 0
    , x_expand = c(-0.1,-0.1)
  ) +
  facet_grid(
    rows = vars(software)
    , cols = vars(depth_maps_generation_quality)
  ) +
  labs(
    subtitle = "posterior predictive distribution of group constrasts with 95% & 50% HDI\nby dense cloud quality and software"
  )
  
ggplot2::ggsave(
  "../data/qlty_fltr_sftwr_comp_mod6.png"
  , plot = ggplot2::last_plot() + labs(subtitle = "")
  , height = 7, width = 10.5
)
```

#### Quality - main effect

let's collapse across the story, filtering mode, software, and study site to compare the dense cloud quality setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_quality) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_quality)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_quality
      , fill = depth_maps_generation_quality
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "inferno", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "quality", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

we can also perform pairwise comparisons

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(depth_maps_generation_quality) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod6, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_quality)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_quality
      , comparison = 
        contrast_list
        # tidybayes::emmeans_comparison("revpairwise") 
        #"pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_quality)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_quality)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      )
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "quality"
)

```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast) %>% 
  dplyr::select(contrast, value, .lower, .upper, pr_gt_zero) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "quality contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()
```

#### Filtering - main effect

let's collapse across the story, dense cloud quality, software, and study site to compare the filtering mode setting effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6
    # this part is crucial
    , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = depth_maps_generation_filtering_mode
      , fill = depth_maps_generation_filtering_mode
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "plasma", drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "filtering mode", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

we can also perform pairwise comparisons

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(depth_maps_generation_filtering_mode) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod6, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | depth_maps_generation_filtering_mode)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = depth_maps_generation_filtering_mode
      , comparison = "pairwise"
    ) %>% 
    dplyr::rename(contrast = depth_maps_generation_filtering_mode)

# separate contrast
brms_contrast_temp = brms_contrast_temp %>% 
  dplyr::ungroup() %>% 
  tidyr::separate_wider_delim(
    cols = contrast
    , delim = " - "
    , names = paste0(
        "sorter"
        , 1:(max(stringr::str_count(brms_contrast_temp$contrast, "-"))+1)
      )
    , too_few = "align_start"
    , cols_remove = F
  ) %>% 
  dplyr::filter(sorter1!=sorter2) %>% 
  dplyr::mutate(
    dplyr::across(
      tidyselect::starts_with("sorter")
      , .fns = function(x){factor(
        x, ordered = T
        , levels = levels(ptcld_validation_data$depth_maps_generation_filtering_mode)
      )}
    )
    , contrast = contrast %>% 
      forcats::fct_reorder(
        paste0(as.numeric(sorter1), as.numeric(sorter2)) %>% 
        as.numeric()
      ) %>% 
      # re order for filtering mode
      forcats::fct_rev()
  ) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast) %>% 
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "filtering mode"
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast) %>% 
  dplyr::select(contrast, value, .lower, .upper, pr_gt_zero) %>% 
   
kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "filtering mode contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()
```

#### Software - main effect

to address one of our main questions, let's also collapse across the story, study site, dense cloud quality, and filtering mode setting to compare the software effect.
In a hierarchical model structure, we have to make use of the `re_formula` argument within `tidybayes::add_epred_draws`

```{r}
ptcld_validation_data %>% 
  dplyr::distinct(software) %>% 
  tidybayes::add_epred_draws(
    brms_f_mod6
    # this part is crucial
    , re_formula = ~ (1 | software)
  ) %>% 
  dplyr::rename(value = .epred) %>% 
  # plot
  ggplot(
    mapping = aes(
      x = value, y = software
      , fill = software
    )
  ) +
    tidybayes::stat_halfeye(
      point_interval = median_hdi, .width = .95
      , interval_color = "gray66"
      , shape = 21, point_color = "gray66", point_fill = "black"
      , justification = -0.01
    ) +
    scale_fill_viridis_d(option = "rocket", begin = 0.3, end = 0.9, drop = F) +
    scale_x_continuous(breaks = scales::extended_breaks(n=8)) +
    labs(
      y = "software", x = "F-score"
      , subtitle = "posterior predictive distribution of F-score with 95% HDI"
      , caption = form_temp
    ) +
    theme_light() +
    theme(legend.position = "none")
```

we can also perform pairwise comparisons

```{r}
brms_contrast_temp =
  ptcld_validation_data %>%
    dplyr::distinct(software) %>% 
    tidybayes::add_epred_draws(
      brms_f_mod6, allow_new_levels = T
      # this part is crucial
      , re_formula = ~ (1 | software)
    ) %>% 
    dplyr::rename(value = .epred) %>% 
    tidybayes::compare_levels(
      value
      , by = software
      , comparison = "pairwise"
    ) %>% 
    dplyr::rename(contrast = software) %>% 
  # median_hdi summary for coloring 
  dplyr::group_by(contrast) %>%
  make_contrast_vars()

# what?
brms_contrast_temp %>% dplyr::glimpse()
```

plot it

```{r}
plt_contrast(
  brms_contrast_temp, caption_text = form_temp
  , y_axis_title = "software"
)
```

and summarize these contrasts

```{r}
brms_contrast_temp %>%
  dplyr::group_by(contrast, pr_gt_zero) %>% 
  tidybayes::median_hdi(value) %>% 
  dplyr::arrange(contrast) %>% 
  dplyr::select(contrast, value, .lower, .upper, pr_gt_zero) %>% 
   
  kableExtra::kbl(
    digits = 2
    , caption = "brms::brm model: 95% HDI of the posterior predictive distribution of group constrasts"
    , col.names = c(
      "software contrast"
      , "difference (F-score)"
      , "HDI low", "HDI high"
      , "Pr(diff>0)"
    )
  ) %>% 
  kableExtra::kable_styling()
```

#### $\sigma$ posteriors

Finally, we can quantify the variation in F-score by comparing the $\sigma$ (`sd`) posteriors

*unsure about the scale of the $\sigma$ parameters are on in the beta model. Here, we invert the logit `sd` values from the model using `plogis()` which converts the parameter values to a probability/proportion (e.g.; 0-1) because they are parameters of the intercept and interaction effects so must be on the transformed (`link = "logit"`) scale...double check*

For a phenomenally excellent overview of binary logistic regression and how to interpret coefficients, see Steven Miller’s most excellent lab [script here](https://post8000.svmiller.com/lab-scripts/logistic-regression-lab.html)

```{r}
# tidybayes::get_variables(brms_f_mod6)
# extract the posterior draws
brms::as_draws_df(brms_f_mod6) %>% 
  dplyr::select(c(tidyselect::starts_with("sd_"))) %>% 
  tidyr::pivot_longer(dplyr::everything()) %>% 
  # dplyr::group_by(name) %>% 
  # tidybayes::median_hdi(value) %>% 
  dplyr::mutate(
    name = name %>% 
      stringr::str_replace_all("depth_maps_generation_quality", "quality") %>% 
      stringr::str_replace_all("depth_maps_generation_filtering_mode", "filtering") %>% 
      forcats::fct_reorder(value)
    , value = plogis(value)
  ) %>%
# plot
  ggplot(aes(x = value, y = name)) +
  tidybayes::stat_dotsinterval(
    point_interval = median_hdi, .width = .95
    , justification = -0.04
    , shape = 21 #, point_size = 3
    , quantiles = 100
  ) +
  labs(x = "", y = "", caption = form_temp) +
  theme_light()
```

Determining overstory or understory seems to have the strongest effect on our ability to accurately detect trees but this effect is highly uncertain. Variance of study site is stronger than variance of depth map generation quality, but the posterior predictive distributions overlap a good deal. The study site (the "subjects" in our study) seems to have the overall strongest effect, but this comes with high uncertainty. Taken alone, the influence of quality, filtering, and software comes with huge uncertainty. This makes sense as the influence of software largely depends on the depth map generation quality, of which we are fairly certain. Filtering mode has the overall weakest effect on tree detection and this comes with relatively high certainty, especially conditional on the depth map generation quality.

and perform model selection via the [R-squared for Bayesian](https://stat.columbia.edu/~gelman/research/published/bayes_R2_v3.pdf) regression models using `brms::bayes_R2()` since we can't compare models with different number of observations using information criteria (we could also use the p-value approach as done above)

```{r}
brms_f_mod6 = brms::add_criterion(brms_f_mod6, criterion = c("loo", "waic"))
# r-squared
dplyr::bind_rows(
  brms::bayes_R2(brms_f_mod6) %>% dplyr::as_tibble() %>% dplyr::mutate(model = "brms_f_mod6") %>% dplyr::relocate(model)
  , brms::bayes_R2(brms_f_mod5) %>% dplyr::as_tibble() %>% dplyr::mutate(model = "brms_f_mod5") %>% dplyr::relocate(model)
) %>% 
dplyr::rename_with(tolower) %>% 
dplyr::arrange(desc(estimate)) %>% 
dplyr::rename(R2.estimate = estimate)
```

Our model without the effects of overstory/understory is a better parsimonious fit of the data in this case...let's roll with `brms_f_mod5` [model](#f_5_pred_mod_bays) for the research article even though our model including overstory/understory has less dispersion (see our $\phi$ [posterior](#f_6_pred_mod_bays))

```{r, warning=FALSE, message=FALSE, echo=FALSE, include=FALSE}
remove(list = ls()[grep("_temp",ls())])
gc()
```